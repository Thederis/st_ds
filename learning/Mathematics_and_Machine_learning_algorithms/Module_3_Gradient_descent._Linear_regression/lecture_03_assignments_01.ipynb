{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем проверять задания:\n",
    "- **Перезапустите ядро** (**restart the kernel**) (В меню, выбрать Ядро (Kernel) $\\rightarrow$ Перезапустить (Restart)\n",
    "- Затем **Выполнить все ячейки**  **run all cells** (В меню, выбрать Ячейка (Cell) $\\rightarrow$ Запустить все (Run All).\n",
    "\n",
    "Убедитесь, что заполнены все ячейки с комментарием \"НАЧАЛО ВАШЕГО РЕШЕНИЯ\".\n",
    "\n",
    "После ячеек с заданием следуют ячейки с проверкой с помощью assert.\n",
    "\n",
    "Если в коде есть ошибки, assert выведет уведомление об ошибке.\n",
    "\n",
    "Если в коде нет ошибок, assert отработает без вывода дополнительной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "464f7145bea12225742468492f043393",
     "grade": false,
     "grade_id": "cell-f809056c43486fdd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы на практике разберемся, как устроен градиентный спуск на примере реализации функционала ошибки Хьюбера с одним оптимизируемым параметром.\n",
    "\n",
    "Также мы визуализируем концепцию регуляризации, основанной на добавлении слагаемого к функции потерь модели.\n",
    "\n",
    "Также мы с нуля реализуем алгоритм линейной регрессии с помощью numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d16dfacc8b031781de005d09b90a231",
     "grade": false,
     "grade_id": "cell-4ae25d5c403491ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Часть 1. Градиентный спуск на примере реализации функционала ошибки Хьюбера с одним оптимизируемым параметром\n",
    "\n",
    "Задачи:\n",
    "- реализовать функцию потерь Хубера\n",
    "- реализовать функцию вычисления производной функции потерь Хубера\n",
    "- визуализировать поведение алгоритма градиентного спуска при различных параметрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62486e6720c1f6a55a9327feb79f836d",
     "grade": false,
     "grade_id": "cell-be62097ea8040575",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rc('lines', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29bbd162ca4025dc881e5ba17eacba26",
     "grade": false,
     "grade_id": "cell-8d89536bfedca827",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Реализуйте в ячейке ниже следующий алгоритм для вычисления значений функции Хьюбера:\n",
    "\n",
    "Если x по модулю меньше или равен delta, то вернуть $\\frac{1}{2} \\cdot x^2$\n",
    "\n",
    "Иначе вернуть $delta \\cdot ( abs(x) - 0.5 \\cdot delta )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de777aaee3bfd5ac4ce2f473144d286c",
     "grade": false,
     "grade_id": "cell-abe59f7dfe94da0b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def huber(x, delta=1.0):\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError()\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "599485960c5725de9f44c8157e45822c",
     "grade": true,
     "grade_id": "cell-5c7e2d0107a66ae3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(huber(0), 0)\n",
    "assert np.isclose(huber(1), 0.5)\n",
    "assert np.isclose(huber(-1), 0.5)\n",
    "assert np.isclose(huber(2), 1.5)\n",
    "assert np.isclose(huber(-2), 1.5)\n",
    "assert np.isclose(huber(10, delta=5), 37.5)\n",
    "assert np.isclose(huber(-2, delta=5), 2.0)\n",
    "assert np.isclose(huber(0.5), 0.125)\n",
    "assert np.isclose(huber(-0.5), 0.125)\n",
    "assert np.isclose(huber(-10, delta=5), 37.5)\n",
    "assert np.isclose(huber(2, delta=5), 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f02dc1c87e5a95bf01b36528050d85a",
     "grade": false,
     "grade_id": "cell-76ef0881e04b0e88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Реализуйте в ячейке ниже следующий алгоритм для вычисления значений производной функции Хьюбера по параметру x:\n",
    "\n",
    "вернуть x, если |x| <= delta\n",
    "\n",
    "Иначе:\n",
    "$\\begin{cases}\n",
    "delta, & \\text{если } x > 0 \\\\\n",
    "-delta, & \\text{если } x < 0 \\\\\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dd97ac0868c399b3aac36d99338f890",
     "grade": false,
     "grade_id": "cell-a1c3b912fe9a6c35",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def huber_derivative(x, delta):\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError()\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7549746b0be9b95382a0e17e29fb95ce",
     "grade": true,
     "grade_id": "cell-1e04534d4ac370af",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert huber_derivative(0, 1) == 0\n",
    "assert huber_derivative(1, 1) == 1\n",
    "assert huber_derivative(-1, 1) == -1\n",
    "assert huber_derivative(2, 1) == 1\n",
    "assert huber_derivative(-2, 1) == -1\n",
    "assert huber_derivative(0.5, 1) == 0.5\n",
    "assert huber_derivative(-0.5, 1) == -0.5\n",
    "assert huber_derivative(2, 5) == 2\n",
    "assert huber_derivative(-2, 5) == -2\n",
    "assert huber_derivative(4, 2) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e2c63496817e60362ad1ad9e34c5867",
     "grade": false,
     "grade_id": "cell-c4f794ae062ac315",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(ax,\n",
    "        x0 = 5,\n",
    "        delta = 1.0,\n",
    "        alpha = 0.1,\n",
    "        eps = 0.01,\n",
    "        n_iter = 100\n",
    "        ):\n",
    "    \"\"\"\n",
    "    :param x0: Стартовая точка для параметра x, выбирать из диапазона [-5, 5] \n",
    "    :param delta: Параметр delta для функции Хубера, 0.0 по умолчанию\n",
    "    :param alpha: Скорость обучения, 0.1 по умолчанию\n",
    "    :param eps: Окрестность оптимума, при попадании в которую прекращается выполнение алгоритма, 0.01 по умолчанию\n",
    "    :param n_iter: Количество итераций до остановки, 100 по умолчанию\n",
    "    :param ax: Объект осей (axis), представляет графический контейнер для отображения данных на графике\n",
    "    \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    y_true = 0.0\n",
    "    y_pred0 = huber(x0, delta=1.0)\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    # Запускаем алгоритм градиентного спуска\n",
    "    for i in range(n_iter):\n",
    "        # Вычисляем производную функции потерь Хубера\n",
    "        grad = huber_derivative(x0, delta)\n",
    "        x1 = x0 - alpha*grad  # Вычисляем новое приближение\n",
    "        y_pred1 = huber(x1, delta=1.0)\n",
    "        # Рисуем отрезок\n",
    "        ax.plot([x0, x1], [y_pred0, y_pred1], 'ro-')\n",
    "        if abs(x1 - x0) < eps:  # Проверяем условие выхода\n",
    "            print(f\"Алгоритм сошелся за {i+1} шагов\")\n",
    "            converged = True\n",
    "            break\n",
    "        x0 = x1\n",
    "        y_pred0 = y_pred1\n",
    "        \n",
    "    if not converged:\n",
    "        print(f\"Алгоритм не сошелся за {i+1} шагов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# # # Рисуем точку минимума\n",
    "ax.plot(0, 0, 'go', markersize=10)\n",
    "\n",
    "# Задаем интервал поиска минимума\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Рисуем график функции Хубера\n",
    "y = np.vectorize(huber)(x)\n",
    "ax.plot(x, y, 'b')\n",
    "\n",
    "gradient_descent(ax,\n",
    "                x0 = 5,\n",
    "                alpha = 1,\n",
    "                eps = 0.01\n",
    "                )\n",
    "# # Настраиваем оси и выводим график на экран\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(0, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "410bc71224743fe50f60eb9408de9827",
     "grade": false,
     "grade_id": "cell-7f63c9cb50dc6113",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!!! Ответьте на следующие вопросы:\n",
    "Вопрос 1: За какое количество шагов сойдется алгоритм, если передать в gradient_descent в ячейке выше\n",
    "такие параметры: (ax, x0 = 5, alpha = 1, eps = 0.01)\n",
    "ОТВЕТ ЗАПИСАТЬ В ПЕРЕМЕННУЮ answer_1_num_steps\n",
    "ПРИМЕР:\n",
    "answer_1_num_steps = 1\n",
    "\"\"\"\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError()\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "\"\"\"\n",
    "Вопрос 2: Сойдется ли алгоритм, если передать в gradient_descent\n",
    "такие параметры: (ax, x0 = -5, alpha = 3, eps = 0.01)\n",
    "Варианты ответов: \"Да\" или \"Нет\"\n",
    "ОТВЕТ ЗАПИСАТЬ В ПЕРЕМЕННУЮ answer_2_converged_or_not\n",
    "ПРИМЕР:\n",
    "answer_2_converged_or_not = \"Да\"\n",
    "\"\"\"\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError()\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e2cf9c90e26b4c551d1c7730b37604a",
     "grade": true,
     "grade_id": "cell-8eabefa45e3ccd9b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e8f2e2c8a40259c534f192134d050e9",
     "grade": false,
     "grade_id": "cell-5a7c9e406c8f29cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Часть 2. Визуализация концепции регуляризации, основанной на добавлении слагаемого к функции потерь модели\n",
    "\n",
    "Реальные данные распределены вдоль линии синусоиды. В данной задаче визуализируеся обобщающая способность модели и регуляризация модели.\n",
    "\n",
    "Задачи:\n",
    "- визуализировать регуляризацию при различных параметрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2093a30e6e03b1c795478bb1a65c77c",
     "grade": false,
     "grade_id": "cell-09218a74fe2bdd09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Задаем точки для визуализации\n",
    "\n",
    "dots = [(-1.9933147173759478, -0.9390893949143323),\n",
    "         (-1.8783910144218139, -0.8667300263876551),\n",
    "         (-1.6698257757272743, -1.0880645654104326),\n",
    "         (-1.4484912367044975, -1.109346732624161),\n",
    "         (-1.4612605370327347, -0.9092943608151123),\n",
    "         (-1.2, -1),\n",
    "         (-0.2524334392929545, -0.16867494177735687),\n",
    "         (-0.060893934369397745, -0.2154957096475598),\n",
    "         (0.4, 0.2),\n",
    "         (0.4158266112181212, 0.46553364119175555),\n",
    "         (1.0330094604162485, 0.7805097159549389)]\n",
    "\n",
    "# создание списков координат в формате, требуемом для matplotlib\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "for el in dots:\n",
    "    x_coords.append(el[0])\n",
    "    y_coords.append(el[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b1cca0e86591a3cdadf997a22689aee",
     "grade": false,
     "grade_id": "cell-c9436120a6949905",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Создайте список degs для итерирования по значениям степени, в нем должно быть 3 элемента: 2, 5, 8\n",
    "\"\"\"\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError()\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5398f74d11a0837a7ef225d430293d65",
     "grade": true,
     "grade_id": "cell-cef5a095de121c99",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d3deefb4b03aee58c8864a9d317e09c",
     "grade": false,
     "grade_id": "cell-b1638301e265518c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "degs_dict = {}\n",
    "\n",
    "for deg in degs:\n",
    "    # Находим коэффициенты полинома deg степени, который проходит через наши точки\n",
    "    coefficients = np.polyfit(x_coords, y_coords, deg=deg)\n",
    "\n",
    "    # # Создаем новый массив x, через которые мы хотим построить линию\n",
    "    new_x_coords = np.linspace(-2, 1, 100)\n",
    "\n",
    "    # Находим соответствующие y-координаты для новых значений x с помощью polyval\n",
    "    new_y_coords = np.polyval(coefficients, new_x_coords)\n",
    "    \n",
    "    degs_dict[deg] = {\"new_x_coords\": new_x_coords,\n",
    "                      \"new_y_coords\": new_y_coords}\n",
    "\n",
    "# Создание графиков\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].plot(x_coords, y_coords, 'bo')\n",
    "    axs[i].plot(degs_dict[degs[i]][\"new_x_coords\"], degs_dict[degs[i]][\"new_y_coords\"], 'r')\n",
    "    axs[i].set_title(f'Степень {degs[i]}')\n",
    "\n",
    "# Настройка общего заголовка\n",
    "fig.suptitle('Демонстрация принципа регуляризации')\n",
    "\n",
    "# Отображение графиков\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22ea23b51a0e36edce3483bcfb844e13",
     "grade": false,
     "grade_id": "cell-c1aa1290c9372201",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Часть 3. Алгоритм линейной регрессии \n",
    "\n",
    "Линейная регрессия - это метод машинного обучения, который используется для прогнозирования числовых значений на основе входных данных. В этом методе используется линейная комбинация входных переменных, чтобы получить выходное значение.\n",
    "\n",
    "Градиентный спуск - это метод оптимизации, который используется для нахождения минимума функции путем итеративного изменения параметров функции в направлении наискорейшего убывания градиента.\n",
    "\n",
    "В данном примере мы рассмотрим линейную регрессию с нуля с градиентным спуском с использованием библиотеки numpy.\n",
    "\n",
    "Задачи:\n",
    "- реализовать функцию вычисления градиента и сохранения истории обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6949c6dbddef158df2199f76c142ac",
     "grade": false,
     "grade_id": "cell-a14956d4010405e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Наша цель в построении модели линейной регрессии - найти оптимальные значения параметров модели (w и b), \n",
    "чтобы минимизировать среднеквадратичную ошибку (MSE) нашей модели.\n",
    "\n",
    "Для этого мы определяем функцию потерь, которая будет вычислять MSE:\n",
    "# Внутри функции compute_cost произведите следующие шаги:\n",
    "    1. вычислите длину y (можно воспользоваться len()) и сохратине эту длину в переменную n\n",
    "    2. вычислите произведение матриц X и theta и сохраните получившуюся матрицу в переменную predictions\n",
    "    3. сохраните в переменную cost вычисления 1/2 * 1/n * сумма квадратов разностей \n",
    "    прогнозов от истинных ответов (можно воспользоваться np.sum и np.square)\n",
    "    ###\n",
    "    ПОЯСНЕНИЕ К ФОРМУЛЕ cost:\n",
    "    Коэффициент 1/2 перед квадратичной функцией потерь часто используется для удобства и облегчения \n",
    "    математических вычислений.\n",
    "    При использовании квадратичной функции потерь вида L = (1/2) * (y - f(x))^2, \n",
    "    где y представляет собой истинное значение, а f(x) - предсказанное значение модели для входных \n",
    "    данных x, коэффициент 1/2 добавляется, чтобы упростить дальнейшие математические выкладки.\n",
    "    Почему используется коэффициент 1/2? При нахождении минимума функции потерь по параметрам \n",
    "    модели необходимо взять производную. Коэффициент 1/2 упрощает дифференцирование и устраняет \n",
    "    множитель 2, который возникает при дифференцировании квадрата.\n",
    "    Таким образом, добавление коэффициента 1/2 перед квадратичной функцией потерь не \n",
    "    влияет на ее форму и свойства, но упрощает математические вычисления.\n",
    "    ###\n",
    "    4. Верните cost через return\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError()\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62ba48f911a1a7294b76909e96bc5cbc",
     "grade": true,
     "grade_id": "cell-b81f55544a734768",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# тестовая выборка\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[7], [9], [11]])\n",
    "theta = np.array([[1], [1]])\n",
    "\n",
    "# тест 1\n",
    "assert np.allclose(compute_cost(X, y, theta), 3.333333333333333)\n",
    "\n",
    "# тест 2\n",
    "theta = np.array([[0], [0]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 41.83333333333333)\n",
    "\n",
    "# тест 3\n",
    "theta = np.array([[1], [2]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 7.333333333333333)\n",
    "\n",
    "# тест 4\n",
    "theta = np.array([[-1], [-1]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 140)\n",
    "\n",
    "# тест 5\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([[7], [8]])\n",
    "theta = np.array([[1], [1], [1]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 12.5)\n",
    "\n",
    "# тест 6\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([[7], [8]])\n",
    "theta = np.array([[0], [0], [0]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 28.25)\n",
    "\n",
    "# тест 7\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([[7], [8]])\n",
    "theta = np.array([[1], [2], [3]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 156.25)\n",
    "\n",
    "# тест 8\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[7], [9], [11]])\n",
    "theta = np.array([[2], [2]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 24.5)\n",
    "\n",
    "# тест 9\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[7], [9], [11]])\n",
    "theta = np.array([[-2], [-2]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 297.8333333333333)\n",
    "\n",
    "# тест 10\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[7], [9], [11]])\n",
    "theta = np.array([[-1], [0]])\n",
    "assert np.allclose(compute_cost(X, y, theta), 77.33333333333333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb58bec4654992a3d8421bc28ee46938",
     "grade": false,
     "grade_id": "cell-5ba1e0e11fc9bbbc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Теперь мы можем использовать градиентный спуск для нахождения оптимальных значений параметров модели. \n",
    "# Для этого мы будем итеративно обновлять значения параметров, используя градиент функции потерь. \n",
    "# Реализуем это через функцию gradient_descent:\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    \"\"\"    \n",
    "    1.1 вычислите длину y (можно воспользоваться len()) и сохратине эту длину в переменную n\n",
    "    1.2 создайте матрицу cost_history с помощью np.zeros, передав в np.zeros iterations в качестве параметра\n",
    "    1.3 создайте матрицу theta_history с помощью np.zeros, передав в np.zeros (iterations, theta.shape[0]) \n",
    "    в качестве параметра. ВНИМАНИЕ: в np.zeros() передается ОДИН параметр - кортеж.\n",
    "    Пример: np.zeros((element1, element2))\n",
    "    \"\"\"\n",
    "\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError()\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
    "    # Итерируемся по количеству итераций\n",
    "    for i in range(iterations):\n",
    "        \"\"\"        \n",
    "        # 2.1 вычислите произведение матриц X и theta, сохранив результат в переменную predictions\n",
    "        # 2.2 вычислите разность матриц predictions - y , сохранив результат в переменную errors \n",
    "        # 2.3 вычислите градиент как 1/n * (транспонированная X dot errors) в переменную gradient \n",
    "        # 2.4 обновите веса (theta): theta = theta - скорость обучения * градиент\n",
    "        # 2.5 сохраните cost_history[i] с помощью функции compute_cost (cost_history[i] = )\n",
    "        # 2.6 сохраните theta_history[i] с помощью theta.T (theta_history[i] = )\n",
    "        \"\"\"\n",
    "\n",
    "        # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "        raise NotImplementedError()\n",
    "        # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
    "    return theta, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fd7aae57f5c3473fa24cd2d12c06ed1",
     "grade": true,
     "grade_id": "cell-4022a56fd21c213d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Тестовая выборка\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[7], [9], [11]])\n",
    "theta = np.array([[1], [1]])\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Тест 1\n",
    "theta, cost_history, theta_history = gradient_descent(X, y, theta, learning_rate, iterations)\n",
    "assert np.allclose(theta, np.array([[-2.46344196], [3.99753184]]))\n",
    "\n",
    "# Тест 2\n",
    "theta = np.array([[0], [0]])\n",
    "theta, cost_history, theta_history = gradient_descent(X, y, theta, learning_rate, iterations)\n",
    "assert np.allclose(theta, np.array([[-2.51713436], [4.03991894]]))\n",
    "\n",
    "# Тест 3\n",
    "theta = np.array([[1], [2]])\n",
    "theta, cost_history, theta_history = gradient_descent(X, y, theta, learning_rate, iterations)\n",
    "assert np.allclose(theta, np.array([[-2.66475138], [4.15645417]]))\n",
    "\n",
    "# Тест 4\n",
    "theta = np.array([[-1], [-1]])\n",
    "theta, cost_history, theta_history = gradient_descent(X, y, theta, learning_rate, iterations)\n",
    "assert np.allclose(theta, np.array([[-2.57082676], [4.08230603]]))\n",
    "\n",
    "# Тест 5\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([[7], [8]])\n",
    "theta = np.array([[1], [1], [1]])\n",
    "learning_rate = 0.001\n",
    "iterations = 10000\n",
    "theta, cost_history, theta_history = gradient_descent(X, y, theta, learning_rate, iterations)\n",
    "assert np.allclose(theta, np.array([[-2.86481901], [0.13770713], [3.14023328]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3b69dcf67b4bad432da4623caf2280b",
     "grade": false,
     "grade_id": "cell-d3017ae2bf72a931",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Для примера, мы создадим простой набор данных с помощью функции numpy\n",
    "# Здесь мы создаем 100 случайных значений для входных данных X и вычисляем выходные данные y, \n",
    "# используя линейную комбинацию с небольшим шумом.\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "theta = np.random.randn(2,1) # случайная инициализация весов\n",
    "X_b = np.c_[np.ones((len(X),1)), X] # добавление фиктивного признака, равного 1, для всех обучающих примеров\n",
    "\n",
    "theta, cost_history, theta_history = gradient_descent(X_b, y, theta, learning_rate=0.1, iterations=50)\n",
    "\n",
    "plt.plot(range(len(cost_history)), cost_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e56dd886b3d2073ba85a69c5e9796bb",
     "grade": false,
     "grade_id": "cell-2e037bd9391c63c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Мы видим, что обучение прошло успешно, значение функционала ошибки снижается во время обучения."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
