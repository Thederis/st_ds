{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "\n",
    "- [Вектор градиента](#gradient)\n",
    "- [Формула градиента линейной регрессии](#gradient_linear_regression)\n",
    "- [Аналитическое решение](#analitic)\n",
    "- [Отладчик](#debugger)\n",
    "- [Реализация в Python](#linear_regression_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вектор градиента  <a class=\"anchor\" id=\"gradient\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор градиента - это вектор, который указывает направление наибольшего увеличения функции, а его длина определяет скорость роста функции в этом направлении. В машинном обучении вектор градиента используется для оптимизации функции потерь, которая является мерой различия между предсказанными моделью значениями и фактическими значениями.\n",
    "\n",
    "В частности, в градиентном спуске используется вектор градиента для нахождения минимума функции потерь путем итеративного изменения параметров модели в направлении, противоположном вектору градиента. Каждый шаг определяется размером шага (learning rate), который определяет, насколько сильно параметры должны быть изменены в каждой итерации.\n",
    "\n",
    "Обозначение вектора градиента по частным производным выглядит так:\n",
    " \n",
    "$$\\nabla f = \\left(\\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\dots, \\frac{\\partial f}{\\partial w_n}\\right)$$\n",
    "\n",
    "где $f$ - функция нескольких переменных, $w_1, w_2, \\dots, w_n$ - ее аргументы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/everest_gradient_red_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/mse_optimized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/regression_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/gradient_orthogonal_to_contour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/gradient_direction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формула градиента линейной регрессии  <a class=\"anchor\" id=\"gradient_linear_regression\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула для градиента линейной регрессии:\n",
    "```python\n",
    "    gradient = (1/n) * X.T.dot(errors)\n",
    "```\n",
    "Давайте посмотрим, как она выводится для MSE.\n",
    "\n",
    "Пусть у нас есть обучающий набор данных с $n$ наблюдениями и $d$ признаками. Каждое наблюдение $i$ имеет $d$ признаков $x_{i1}, x_{i2}, ..., x_{id}$ и соответствующий целевой признак $y_i$. Задача линейной регрессии состоит в том, чтобы найти линейную функцию, которая наилучшим образом соответствует данным. Мы можем записать модель линейной регрессии следующим образом:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(x) = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d = \\sum_{j=0}^{d-1} w_j x_j,\n",
    "$$\n",
    "\n",
    "где $\\hat{y}$ - это предсказанный ответ, $w_j$ - это веса (коэффициенты) для каждого признака, а $x_j$ - это значение признака $j$ для данного наблюдения.\n",
    "\n",
    "MSE это среднее значение квадрата разности между предсказанным значением $\\hat{y_i}$ и истинным значением $y_i$ для каждого наблюдения $i$ в обучающем наборе:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i)^2.\n",
    "$$\n",
    "\n",
    "Теперь мы можем найти градиент MSE по весам $w_j$, чтобы обновить их в процессе обучения. Градиент MSE по $w_j$ можно записать следующим образом:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i)^2.\n",
    "$$\n",
    "\n",
    "Далее, мы можем использовать правило цепочки, чтобы найти этот градиент:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i) \\frac{\\partial \\hat{y_i}}{\\partial w_j}.\n",
    "$$\n",
    "\n",
    "Теперь нам нужно найти производную $\\frac{\\partial \\hat{y_i}}{\\partial w_j}$. Мы можем заметить, что $\\hat{y_i}$ является линейной комбинацией весов $w_j$ и признаков $x_j$, поэтому производная по $w_j$ просто равна соответствующему признаку $x. \n",
    "\n",
    "Если $j=0$, то\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\left(\\sum_{k=0}^d w_k x_{ik}\\right) = \\frac{\\partial}{\\partial w_0} (w_0 + w_1 x_{i1} + \\cdots + w_d x_{id}) = 1.\n",
    "$$\n",
    "\n",
    "Если $j \\neq 0$, то\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\left(\\sum_{k=0}^d w_k x_{ik}\\right) = \\frac{\\partial}{\\partial w_j} (w_0 x_{i0} + w_1 x_{i1} + \\cdots + w_j x_{ij} + \\cdots + w_d x_{id}) = x_{ij}.\n",
    "$$\n",
    "\n",
    "Таким образом, мы можем записать градиент MSE по $w_j$ следующим образом:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i) x_{ij}, \\qquad j = 0, 1, \\ldots, d-1.\n",
    "$$\n",
    "\n",
    "Эта формула градиента позволяет обновлять веса $w_j$ в процессе обучения, используя градиентный спуск или его вариации. В частности, мы можем использовать эту формулу для обучения линейной регрессии методом наименьших квадратов или стохастическим градиентным спуском."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Простейший случай: смещение и один признак**\n",
    "\n",
    "Модель: $m(x) = w_0 + w_1 x$\n",
    "\n",
    "Параметры: $w_0 , w_1$\n",
    "\n",
    "Функционал ошибки: $Q(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i)^2$\n",
    "\n",
    "Первая компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_0} = \\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) 1$\n",
    "\n",
    "Первая компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_1} = \\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) x_{i1}$\n",
    "\n",
    "Вектор градиента: $\\nabla Q(w_0, w_1) = (\\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) 1, \\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) x_{i1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общий случай: d признаков**\n",
    "\n",
    "Модель: $m(x) = w_0 + w_1 x_1 + \\dots + w_{d - 1} x_{d - 1}$\n",
    "\n",
    "Параметры: $w_0, w_1, \\dots , w_{d - 1}$\n",
    "\n",
    "Функционал ошибки: $Q(w_0, \\dots, w_{d - 1}) = \\frac{1}{n} \\sum_{i=1}^n (w_0 \\cdot 1 + w_1 \\cdot 1 x_{i1} + \\dots +  w_{d-1} \\cdot x_{id-1} - y_i)^2$\n",
    "\n",
    "Первая компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_0} = \\frac{2}{n} \\sum_{i=1}^n (w_0 \\cdot 1 + w_1 \\cdot 1 x_{i1} + \\dots +  w_{d-1} \\cdot x_{id-1} - y_i) 1$\n",
    "\n",
    "...\n",
    "\n",
    "d компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_{d-1}} = \\frac{2}{n} \\sum_{i=1}^n (w_0 \\cdot 1 + w_1 \\cdot 1 x_{i1} + \\dots +  w_{d-1} \\cdot x_{id-1} - y_i) x_{id-1}, \\qquad j = 0, 1, \\ldots, d-1$\n",
    "\n",
    "Вектор градиента: $\\nabla Q(w) = \\frac{2}{n} X^T (X \\cdot w - y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитическое решение линейной регрессии  <a class=\"anchor\" id=\"analitic\"></a>\n",
    "\n",
    "Функция ошибки: $ (x_i \\cdot w - y_i)^2 $\n",
    "\n",
    "Производная функции ошибки (по правилу производной сложной функции): $ 2 \\cdot (x_i \\cdot w - y_i) \\cdot x_i $\n",
    "\n",
    "Вектор градиента функционала ошибки в матричном виде: $\\nabla Q(w) = \\frac{2}{n} X^T (X \\cdot w - y) $\n",
    "\n",
    "Приравниваем к 0:\n",
    "\n",
    "$\\frac{2}{n} X^T (X \\cdot w - y) = 0$\n",
    "\n",
    "Убираем дробь, умножив обе части на $\\frac{n}{2}$:\n",
    "\n",
    "$X^T (X \\cdot w - y) = 0$\n",
    "\n",
    "Раскрываем скобки и перенеся $X^T y$:\n",
    "\n",
    "$X^T X \\cdot w = X^T y$\n",
    "\n",
    "Решаем относительно w:\n",
    "\n",
    "$w = (X^T X)^{-1} X^T y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация в Python <a class=\"anchor\" id=\"linear_regression_python\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с циклами <a class=\"anchor\" id=\"linear_regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    y_pred = bias\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        y_pred += weights[i] * X[i]\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def linear_regression(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = [0] * n_features\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Инициализация градиентов\n",
    "        dw = [0] * n_features\n",
    "        db = 0\n",
    "        \n",
    "        \"\"\"!!! Обратить внимание на этот цикл - по каждому признаку объекта\n",
    "        происходит изменение соответствующего элемента вектора градиента\n",
    "        \"\"\"\n",
    "        # Проход по каждому объекту \n",
    "        for i in range(n_samples):\n",
    "            # Предсказание\n",
    "            y_pred = predict(X[i], weights, bias)\n",
    "            \n",
    "            # Обновление градиентов\n",
    "            for j in range(n_features):\n",
    "                dw[j] += (y_pred - y[i]) * X[i][j]\n",
    "            db += y_pred - y[i]\n",
    "        \n",
    "        # Обновление весов и смещения\n",
    "        for j in range(n_features):\n",
    "            weights[j] -= (learning_rate * dw[j]) / n_samples\n",
    "        bias -= (learning_rate * db) / n_samples\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [0.8263654277742603, 0.9757570733131978, 1.1251487188521354]\n",
      "Смещение: 0.14939164553893752\n",
      "Предсказанное значение: 32.64815835595235\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с numpy <a class=\"anchor\" id=\"linear_regression_numpy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def linear_regression_numpy(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Предсказание\n",
    "        y_pred = predict(X, weights, bias)\n",
    "\n",
    "        # Обновление градиентов\n",
    "        dw = np.dot(X.T, (y_pred - y)) / n_samples\n",
    "        db = np.mean(y_pred - y)\n",
    "\n",
    "        # Обновление весов и смещения\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [0.82636543 0.97575707 1.12514872]\n",
      "Смещение: 0.14939164553893752\n",
      "Предсказанное значение: 32.64815835595235\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression_numpy(X, y)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с sklearn <a class=\"anchor\" id=\"linear_regression_sklearn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def predict(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def linear_regression(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    weights = model.coef_\n",
    "    bias = model.intercept_\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [1. 1. 1.]\n",
      "Смещение: 1.7763568394002505e-15\n",
      "Предсказанное значение: 33.0\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с torch <a class=\"anchor\" id=\"linear_regression_torch\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /home/denis/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/denis/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /home/denis/.local/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/denis/.local/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/denis/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/denis/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.12.4 fsspec-2023.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.2.140 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [0.82301044 0.9752538  1.127497  ]\n",
      "Смещение: 0.15224327\n",
      "Предсказанное значение: 32.6401032358408\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def linear_regression(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = torch.zeros(n_features, requires_grad=True)\n",
    "    bias = torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Forward pass\n",
    "        y_pred = torch.matmul(X, weights) + bias\n",
    "        \n",
    "        # Вычисление loss\n",
    "        loss = torch.mean((y_pred - y) ** 2)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Обновление весов и смещения\n",
    "        with torch.no_grad():\n",
    "            weights -= learning_rate * weights.grad / n_samples\n",
    "            bias -= learning_rate * bias.grad / n_samples\n",
    "            \n",
    "            # Ручное обнуление градиента\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    \n",
    "    return weights.detach().numpy(), bias.detach().numpy()\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанное значение: tensor([32.1810], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Преобразование данных в тензоры PyTorch\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Определение модели линейной регрессии\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)  # Один входной признак и один выходной признак\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Инициализация модели\n",
    "model = LinearRegression()\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Цикл обучения\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y.view(-1, 1))\n",
    "    \n",
    "    # Backward pass и оптимизация\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = torch.tensor(np.array([10, 11, 12]), dtype=torch.float32)\n",
    "y_pred = model(x_test)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с циклами с L2 регуляризацией <a class=\"anchor\" id=\"linear_regression_l2_regularization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, learning_rate=0.01, regularization=0.01, num_iterations=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        dw = np.zeros(n_features)\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            y_pred = predict(X[i], weights, bias)\n",
    "            \n",
    "            dw += (y_pred - y[i]) * X[i]\n",
    "            db += y_pred - y[i]\n",
    "        \"\"\"!!! Обратить внимание на то, как добавляется регуляризация\"\"\"    \n",
    "        # Пояснение, почему регуляризация добавляется к градиенту\n",
    "        # https://datascience.stackexchange.com/questions/111993/why-would-we-add-regularization-loss-to-the-gradient-itself-in-an-svm\n",
    "        # Берется производная по весам (поэтому weights, а не weights в кквадрате) и коэффициент 1/2 перед производной\n",
    "        dw = (dw / n_samples) + (regularization * weights)\n",
    "        db = db / n_samples\n",
    "        \n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "\n",
    "def predict(X, weights, bias):\n",
    "    y_pred = np.dot(X, weights) + bias\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [0.99707548 0.99936727 1.00165907]\n",
      "Смещение: 0.0051771587861033095\n",
      "Предсказанное значение: 32.98888073086366\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y, learning_rate=0.01, regularization=0.01, num_iterations=1000)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
