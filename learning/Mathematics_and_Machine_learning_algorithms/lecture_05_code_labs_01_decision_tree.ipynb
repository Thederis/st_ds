{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы рассмотрим особенности модели решающего дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h93nb1trtwhu"
   },
   "source": [
    "**Решающее дерево** - это алгоритм машинного обучения, который используется для решения задач классификации и регрессии. Оно представляет собой древовидную структуру, где каждый узел представляет тест на одном из признаков, а каждая ветвь - возможный результат этого теста. Листья дерева представляют собой конечный результат - прогноз для новых данных.\n",
    "\n",
    "В процессе построения решающего дерева, алгоритм выбирает тест, который лучше всего разделяет данные на различные классы или предсказывает значение целевой переменной. Затем данные разбиваются на две или более частей в соответствии с результатами теста. Этот процесс повторяется для каждой полученной части, пока не будет достигнут критерий останова.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация энтропии\n",
    "\n",
    "Энтропийный критерий используется в машинном обучении для измерения неопределенности в наборе данных. Он основан на понятии энтропии из информационной теории.\n",
    "\n",
    "Энтропия является мерой неопределенности. Чем выше энтропия, тем более неопределенным является набор данных. Для бинарной классификации, когда у нас есть два класса, энтропия может быть выражена следующей формулой:\n",
    "\n",
    "$$H(p) = -p_1 \\log_2(p_1) -p_2 \\log_2(p_2)$$\n",
    "\n",
    "где $p_1$ и $p_2$ - вероятности первого и второго классов соответственно.\n",
    "\n",
    "Чтобы применить энтропийный критерий к дереву решений, мы вычисляем энтропию для каждого разбиения на признаке и выбираем разбиение с наименьшей энтропией. Чем меньше энтропия, тем более \"чистым\" является разбиение, то есть чем меньше неопределенности в подмножествах данных, полученных после разбиения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации энтропийного критерия в Python для вектора признаков, нам нужно вычислить энтропию для каждого разбиения и выбрать разбиение с наименьшей энтропией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Вычисляет энтропию вектора y со значениями дискретных переменных.\n",
    "\n",
    "    Аргументы:\n",
    "    - y: вектор numpy с дискретными значениями.\n",
    "\n",
    "    Возвращает:\n",
    "    - entropy: значение энтропии типа float.\n",
    "    \"\"\"\n",
    "    # Подсчитываем количество каждого уникального значения в y.\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    # Вычисляем вероятность каждого уникального значения.\n",
    "    probs = counts / len(y)\n",
    "    # Вычисляем значение энтропии.\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "def find_best_split_entropy(X, y):\n",
    "    \"\"\"\n",
    "    Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, используя критерий энтропии.\n",
    "\n",
    "    Аргументы:\n",
    "    - X: вектор numpy с вещественными значениями признаков.\n",
    "    - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "    Возвращает:\n",
    "    - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "    - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "    - best_gain: значение критерия энтропии для лучшего разбиения.\n",
    "    \"\"\"\n",
    "    best_feature, best_threshold, best_gain = None, None, 0\n",
    "    # Итерируемся по всем признакам.\n",
    "    for feature in range(X.shape[1]):\n",
    "        # Находим уникальные значения признака.\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        # Итерируемся по всем возможным пороговым значениям признака.\n",
    "        for threshold in thresholds:\n",
    "            # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
    "            left_indices = X[:, feature] <= threshold\n",
    "            right_indices = X[:, feature] > threshold\n",
    "            # Пропускаем текущую итерацию, если не найдены объекты, которые относятся к левому или правому поддереву.\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                continue\n",
    "            # Определяем вектор целевой переменной для объектов, которые относятся к левому и правому поддереву.\n",
    "            left_y, right_y = y[left_indices], y[right_indices]\n",
    "            # Вычисляем значение критерия энтропии для текущего разбиения.\n",
    "            gain = entropy(y) - (len(left_y) / len(y)) * entropy(left_y) - (len(right_y) / len(y)) * entropy(right_y)\n",
    "            # Обновляем значения лучшего разбиения, если найдено разбиение с большим значением\n",
    "            if gain > best_gain:\n",
    "                best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "    return best_feature, best_threshold, best_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature: 0\n",
      "Best threshold: 4.0\n",
      "Best gain: 0.31127812445913283\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[2, 4], [2, 6], [2, 8], [4, 10], [6, 10], [8, 8], [8, 6], [8, 4], \n",
    "     [6, 6], [6, 4], [10, 2], [6, 8], [7.683182774875691, 2.0378865840698976], [12, 4], [12, 6], [12, 8]])\n",
    "y = np.array(np.array([0]*8 + [1]*8))\n",
    "best_feature, best_threshold, best_gain = find_best_split_entropy(X, y)\n",
    "print(\"Best feature:\", best_feature)\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "print(\"Best gain:\", best_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Изображение](images05/dividing_plane.png \"Название изображения\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение решающего дерева и линейной модели\n",
    "\n",
    "- Линейные модели могут работать лучше на линейно разделимых данных\n",
    "\n",
    "- При сложных нелинейных зависимостях решающее дерево может работать лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Импорт необходимых библиотек:\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Генерация искусственного набора данных:\n",
    "\"\"\"\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "data = np.zeros(shape=(n, 2))\n",
    "data[:, 0] = np.linspace(-5, 5, n)\n",
    "data[:, 1] = data[:, 0] + 0.5 * np.random.normal(size=n)\n",
    "target = (data[:, 1] > data[:, 0]).astype(int)\n",
    "plt.scatter(data[:, 0], data[:, 1], s=100, c=target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Разделение набора данных на обучающую и тестовую выборки:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Создание объекта линейного классификатора и обучение его на обучающей выборке:\n",
    "\"\"\"\n",
    "linear_classifier = LogisticRegression(random_state=seed)\n",
    "linear_classifier.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Получение прогнозов на тестовой выборке и оценка точности классификатора:\n",
    "\"\"\"\n",
    "y_pred_linear = linear_classifier.predict(X_test)\n",
    "print('f1_score_linear:', f1_score(y_test, y_pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Визуализация разделяющей поверхности линейного классификатора\n",
    "\"\"\"\n",
    "plot_decision_regions(X_test, y_test, linear_classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Создание объекта дерева и обучение его на обучающей выборке:\n",
    "\"\"\"\n",
    "tree_classifier = DecisionTreeClassifier(random_state=seed)\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Получение прогнозов на тестовой выборке и оценка точности дерева:\n",
    "\"\"\"\n",
    "y_pred_tree = tree_classifier.predict(X_test)\n",
    "print('f1_score_tree:', f1_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Визуализация разделяющей поверхности дерева\n",
    "\"\"\"\n",
    "plot_decision_regions(X_test, y_test, tree_classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что деревья решений могут работать хуже на линейно разделимых выборках"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Wrangling & EDA with Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
