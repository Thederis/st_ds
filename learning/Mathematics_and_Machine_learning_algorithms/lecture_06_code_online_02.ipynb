{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2369EvqoDAR9"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpbZzAAZDASA",
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы рассмотрим особенности реализаций градиентного бустинга xgboost, lightgbm и catboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KOy94-ZDASD"
   },
   "source": [
    "Мы будем использовать датасет \"Bank Marketing\". В датасете \"Bank Marketing\" обычно содержатся данные, связанные с маркетинговыми кампаниями банка. Вот некоторые общие колонки, которые могут присутствовать в датасете \"Bank Marketing\":\n",
    "\n",
    "1. **age**: Возраст клиента.\n",
    "\n",
    "2. **job**: Тип работы клиента.\n",
    "\n",
    "3. **marital**: Семейное положение клиента.\n",
    "\n",
    "4. **education**: Образование клиента.\n",
    "\n",
    "5. **default**: Есть ли у клиента кредитный дефолт (задолженность).\n",
    "\n",
    "6. **balance**: Баланс на счете клиента.\n",
    "\n",
    "7. **housing**: Имеет ли клиент ипотеку (жилую недвижимость).\n",
    "\n",
    "8. **loan**: Имеет ли клиент личный заем.\n",
    "\n",
    "9. **contact**: Способ связи с клиентом.\n",
    "\n",
    "10. **day**: День месяца, когда был выполнен последний контакт.\n",
    "\n",
    "11. **month**: Месяц года, когда был выполнен последний контакт.\n",
    "\n",
    "12. **duration**: Продолжительность последнего контакта в секундах.\n",
    "\n",
    "13. **campaign**: Количество контактов, выполненных во время данной кампании.\n",
    "\n",
    "14. **pdays**: Количество дней, прошедших с момента последнего контакта до предыдущей кампании.\n",
    "\n",
    "15. **previous**: Количество контактов, выполненных перед текущей кампанией.\n",
    "\n",
    "16. **poutcome**: Результат предыдущей маркетинговой кампании.\n",
    "\n",
    "17. **target_variable**: Целевая переменная, которую необходимо предсказать. Например, это может быть информация о том, стал ли клиент подписчиком (1) или нет (0) на определенное предложение банка.\n",
    "\n",
    "Обратите внимание, что фактические названия колонок и их типы данных могут различаться в зависимости от конкретного датасета \"Bank Marketing\", поэтому рекомендуется проверить описание колонок в вашем конкретном наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5TI7cZVDMuN",
    "outputId": "15894ae4-f0af-4ad2-f1b7-b4384dd7e841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/denis/.local/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /home/denis/.local/lib/python3.10/site-packages (1.3.1)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.0-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lightgbm\n",
      "  Downloading lightgbm-4.1.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting catboost\n",
      "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting optuna\n",
      "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/denis/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/denis/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/denis/.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/denis/.local/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/denis/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/denis/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: plotly in /home/denis/.local/lib/python3.10/site-packages (from catboost) (5.15.0)\n",
      "Requirement already satisfied: graphviz in /home/denis/.local/lib/python3.10/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /home/denis/.local/lib/python3.10/site-packages (from catboost) (3.7.2)\n",
      "Requirement already satisfied: tqdm in /home/denis/.local/lib/python3.10/site-packages (from hyperopt) (4.66.1)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from hyperopt) (0.18.2)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /home/denis/.local/lib/python3.10/site-packages (from hyperopt) (3.1)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/denis/.local/lib/python3.10/site-packages (from optuna) (23.1)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting colorama>=0.4.6\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/denis/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (612 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.9/612.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib->catboost) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/denis/.local/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/denis/.local/lib/python3.10/site-packages (from matplotlib->catboost) (4.41.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/denis/.local/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/denis/.local/lib/python3.10/site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/denis/.local/lib/python3.10/site-packages (from plotly->catboost) (8.2.2)\n",
      "Installing collected packages: py4j, greenlet, colorlog, colorama, cloudpickle, xgboost, sqlalchemy, lightgbm, hyperopt, catboost, bayesian-optimization, alembic, optuna\n",
      "Successfully installed alembic-1.12.0 bayesian-optimization-1.4.3 catboost-1.2.2 cloudpickle-3.0.0 colorama-0.4.6 colorlog-6.7.0 greenlet-3.0.0 hyperopt-0.2.7 lightgbm-4.1.0 optuna-3.4.0 py4j-0.10.9.7 sqlalchemy-2.0.22 xgboost-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas scikit-learn xgboost lightgbm catboost hyperopt optuna bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_6Eg2YM-DASE"
   },
   "outputs": [],
   "source": [
    "# Библиотеки общего назначения\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Модули метрик\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Модули препроцессинга данных\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Библиотеки градиентного бустинга\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Загрузка данных из CSV файла\n",
    "url = 'https://datahub.io/machine-learning/bank-marketing/r/bank-marketing.csv'\n",
    "column_names = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', \n",
    "                'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', \n",
    "                'poutcome', 'target_variable']\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "data.columns = column_names\n",
    "data.head()\n",
    "\n",
    "# Разделение на признаки (X) и целевую переменную (y)\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable'].apply(lambda x: x - 1)\n",
    "\n",
    "\n",
    "# Разделение на обучающий и тестовый наборы данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Определение препроцессора для категориальных и числовых признаков\n",
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Объединение препроцессоров\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Создание пайплайна для препроцессора\n",
    "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Применение препроцессора к данным\n",
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQUyMMjxDASF"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLLW5M1jDASF"
   },
   "source": [
    "XGBoost (eXtreme Gradient Boosting) - это библиотека градиентного бустинга, предназначенная для решения задач классификации и регрессии. Она основана на идее градиентного бустинга, который строит ансамбль слабых моделей (обычно деревьев решений) и объединяет их для получения более сильной и устойчивой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUjEzU1FDASG"
   },
   "source": [
    "Описание алгоритма XGBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - Каждое дерево строится поэтапно. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - Для построения дерева используется критерий информативности, такой как критерий Джини или энтропийный критерий, для определения наилучшего разбиения на каждом узле дерева.\n",
    "   - Деревья строятся с ограничением на их глубину или другими параметрами, чтобы избежать переобучения.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева. Веса определяются скоростью обучения (learning rate), которая контролирует влияние каждого дерева на итоговое предсказание модели.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - Дополнительные механизмы регуляризации в XGBoost помогают предотвратить переобучение и улучшить обобщающую способность модели.\n",
    "   - XGBoost предлагает несколько методов регуляризации, таких как L1- и L2-регуляризация (также известные как регуляризация Лассо и ридж), которые помогают контролировать сложность модели и предотвращать переобучение. Эти методы добавляют штрафы к функции потерь, которые зависят от весов модели.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - XGBoost использует адаптивную функцию потерь, которая сочетает в себе различные функции потерь в зависимости от значения целевой переменной. Например, для задачи классификации с двумя классами может использоваться кросс-энтропия ($L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$), а для задачи регрессии - среднеквадратичная ошибка $(\\frac{1}{n}\\sum (y - \\hat{y})^2)$.\n",
    "   \n",
    "7. Ансамблирование деревьев:\n",
    "   - Поскольку XGBoost строит ансамбль из нескольких деревьев, предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели. Модель объединяет прогнозы всех деревьев с учетом их весов, определенных на основе ошибок и значимости.\n",
    "\n",
    "8. Прогнозирование:\n",
    "    - После обучения модели XGBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HZwAApPDASG"
   },
   "source": [
    "При оптимизации модели XGBoost обычно оптимизируют следующие основные параметры:\n",
    "\n",
    "1. **max_depth**: Глубина дерева. Этот параметр контролирует, насколько глубоким может быть каждое дерево в ансамбле. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр определяет вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **n_estimators**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bytree**: Доля признаков, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **colsample_bylevel**: Доля признаков, используемых для построения каждого дерева на каждом уровне. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "7. **colsample_bynode**: Доля признаков, используемых для построения каждого дерева в каждом узле разбиения. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "8. **gamma**: Минимальное уменьшение функции потерь, необходимое для выполнения расщепления в дереве. Этот параметр контролирует регуляризацию модели. Большее значение gamma помогает предотвратить переобучение за счет увеличения порога для выполнения расщепления.\n",
    "\n",
    "9. **reg_alpha**: Параметр L1 регуляризации. Он помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов.\n",
    "\n",
    "10. **reg_lambda**: Параметр L2 регуляризации. Он помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8_sZQSDDASH",
    "outputId": "f8de00b5-c5bf-4302-dab9-00a0d2ce7fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время работы xgb_gridsearch_execution_time: 49.16833782196045\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 30}\n",
      "Лучшая F1: 0.4608480661550279\n",
      "F1 на тестовом наборе данных: 0.4515731030228254\n"
     ]
    }
   ],
   "source": [
    "# Библиотеки и модули оптимизации\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Определение модели XGBoost\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [30]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "xgb_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_gridsearch_execution_time:\", xgb_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_gridsearch_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWSQWzqEDASI",
    "outputId": "afbd674c-9c5a-4601-c281-623d357a8b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:50<00:00,  5.03s/trial, best loss: -0.5251141552511416]\n",
      "Время работы xgb_hyperopt_execution_time: 50.32681059837341\n",
      "Лучшие параметры: {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Лучшая F1-мера: 0.5251141552511416\n",
      "F1 на тестовом наборе данных: 0.5251141552511416\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'n_estimators': hp.choice('n_estimators', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "xgb_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_hyperopt_execution_time:\", xgb_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'max_depth': [3, 4, 5][best['max_depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'n_estimators': [30, 50, 100][best['n_estimators']]\n",
    "}\n",
    "# Лучшие параметры и значение метрики\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_hyperopt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyXPDbY9DASI",
    "outputId": "ea6ef89b-1e59-41a6-9228-42c5108d2512"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-09 17:01:24,235] A new study created in memory with name: no-name-3d5656c0-6819-4e48-9724-541a37bf00c7\n",
      "[I 2023-06-09 17:01:28,227] Trial 0 finished with value: 0.43324937027707805 and parameters: {'max_depth': 4, 'learning_rate': 0.019749516996585434, 'n_estimators': 50}. Best is trial 0 with value: 0.43324937027707805.\n",
      "[I 2023-06-09 17:01:31,699] Trial 1 finished with value: 0.4352791878172589 and parameters: {'max_depth': 4, 'learning_rate': 0.07604569613661794, 'n_estimators': 50}. Best is trial 1 with value: 0.4352791878172589.\n",
      "[I 2023-06-09 17:01:37,244] Trial 2 finished with value: 0.49095295536791317 and parameters: {'max_depth': 5, 'learning_rate': 0.09607087080427647, 'n_estimators': 50}. Best is trial 2 with value: 0.49095295536791317.\n",
      "[I 2023-06-09 17:01:44,488] Trial 3 finished with value: 0.5210374639769453 and parameters: {'max_depth': 5, 'learning_rate': 0.09586958138171592, 'n_estimators': 100}. Best is trial 3 with value: 0.5210374639769453.\n",
      "[I 2023-06-09 17:01:50,800] Trial 4 finished with value: 0.35161744022503516 and parameters: {'max_depth': 3, 'learning_rate': 0.02617072923509754, 'n_estimators': 100}. Best is trial 3 with value: 0.5210374639769453.\n",
      "[I 2023-06-09 17:01:55,166] Trial 5 finished with value: 0.4553961322520274 and parameters: {'max_depth': 3, 'learning_rate': 0.0792983569604727, 'n_estimators': 100}. Best is trial 3 with value: 0.5210374639769453.\n",
      "[I 2023-06-09 17:01:57,398] Trial 6 finished with value: 0.40935672514619886 and parameters: {'max_depth': 3, 'learning_rate': 0.018783000841809852, 'n_estimators': 50}. Best is trial 3 with value: 0.5210374639769453.\n",
      "[I 2023-06-09 17:02:03,623] Trial 7 finished with value: 0.3636363636363636 and parameters: {'max_depth': 3, 'learning_rate': 0.04112734672694247, 'n_estimators': 100}. Best is trial 3 with value: 0.5210374639769453.\n",
      "[I 2023-06-09 17:02:05,843] Trial 8 finished with value: 0.34555712270803945 and parameters: {'max_depth': 3, 'learning_rate': 0.05641344949323529, 'n_estimators': 50}. Best is trial 3 with value: 0.5210374639769453.\n",
      "[I 2023-06-09 17:02:07,246] Trial 9 finished with value: 0.39391936549900863 and parameters: {'max_depth': 3, 'learning_rate': 0.05080602772683859, 'n_estimators': 30}. Best is trial 3 with value: 0.5210374639769453.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время работы xgb_optuna_execution_time: 43.012019872665405\n",
      "Лучшие параметры: {'max_depth': 5, 'learning_rate': 0.09586958138171592, 'n_estimators': 100}\n",
      "Лучшая F1-мера: 0.5210374639769453\n",
      "F1 на тестовом наборе данных: 0.5210374639769453\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [30, 50, 100])\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "xgb_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_optuna_execution_time:\", xgb_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_optuna_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_optuna_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gz1EU4dlDASJ",
    "outputId": "0cff63a2-e588-4c23-f12d-d6e3edd3ad9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.4142   \u001b[0m | \u001b[0m0.03808  \u001b[0m | \u001b[0m4.901    \u001b[0m | \u001b[0m81.24    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.3472   \u001b[0m | \u001b[0m0.06027  \u001b[0m | \u001b[0m3.312    \u001b[0m | \u001b[0m40.92    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.5003   \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m4.899    \u001b[0m | \u001b[95m79.93    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.4778   \u001b[0m | \u001b[0m0.07988  \u001b[0m | \u001b[0m4.893    \u001b[0m | \u001b[0m79.92    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.4857   \u001b[0m | \u001b[0m0.09815  \u001b[0m | \u001b[0m4.853    \u001b[0m | \u001b[0m80.03    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.4135   \u001b[0m | \u001b[0m0.03679  \u001b[0m | \u001b[0m4.967    \u001b[0m | \u001b[0m79.98    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.497    \u001b[0m | \u001b[0m0.09629  \u001b[0m | \u001b[0m4.755    \u001b[0m | \u001b[0m80.02    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.4913   \u001b[0m | \u001b[0m0.09163  \u001b[0m | \u001b[0m4.631    \u001b[0m | \u001b[0m80.0     \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.4852   \u001b[0m | \u001b[0m0.08243  \u001b[0m | \u001b[0m4.664    \u001b[0m | \u001b[0m79.89    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.4497   \u001b[0m | \u001b[0m0.0634   \u001b[0m | \u001b[0m4.712    \u001b[0m | \u001b[0m80.17    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.4892   \u001b[0m | \u001b[0m0.0871   \u001b[0m | \u001b[0m4.775    \u001b[0m | \u001b[0m79.93    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.4109   \u001b[0m | \u001b[0m0.03758  \u001b[0m | \u001b[0m4.426    \u001b[0m | \u001b[0m79.98    \u001b[0m |\n",
      "=============================================================\n",
      "Время работы xgb_bayes_opt_execution_time: 62.50949716567993\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 4.899137796523717, 'n_estimators': 79.9290445049531}\n",
      "Лучшая F1-мера: 0.5002935995302408\n",
      "F1-мера на тестовом наборе данных: 0.5002935995302408\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "xgb_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_bayes_opt_execution_time:\", xgb_bayes_opt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных с лучшими параметрами\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = xgb.XGBClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "xgb_bayes_opt_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1-мера на тестовом наборе данных:\", xgb_bayes_opt_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeM4ZGD_DASJ"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCCshNyWDASK"
   },
   "source": [
    "Описание алгоритма LightGBM:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - LightGBM использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - LightGBM использует оптимизированную версию алгоритма градиентного бустинга, которая основана на методе обучения по гистограммам. Это позволяет существенно ускорить процесс построения деревьев.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - LightGBM также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - LightGBM поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - LightGBM также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются для получения итогового предсказания модели.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели LightGBM можно использовать для прогнозирования на новых данных, аналогично XGBoost. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ps-1fi5DASK"
   },
   "source": [
    "При оптимизации модели XGBoost обычно оптимизируют следующие основные параметры:\n",
    "\n",
    "1. **max_depth**: Глубина дерева. Этот параметр контролирует, насколько глубоким может быть каждое дерево в ансамбле. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр определяет вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **n_estimators**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bytree**: Доля признаков, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **min_split_gain**: Минимальное уменьшение функции потерь, необходимое для выполнения расщепления в дереве. Этот параметр контролирует регуляризацию модели. Большее значение min_split_gain помогает предотвратить переобучение за счет увеличения порога для выполнения расщепления.\n",
    "\n",
    "7. **reg_alpha** и **reg_lambda**: Параметры L1 и L2 регуляризации. Они помогают контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5S_6T_wDDASK",
    "outputId": "fa515e60-9828-4224-865e-9558d913b293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время работы lgb_gridsearch_execution_time: 52.43806481361389\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "Лучшая F1: 0.5387241287197826\n",
      "F1 на тестовом наборе данных: 0.5427927927927928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение модели LightGBM\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.001],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "lgb_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_gridsearch_execution_time:\", lgb_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_gridsearch_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zE6TnKb4DASK",
    "outputId": "71269477-feae-4f57-d2b0-186292a9f117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.44trial/s, best loss: -0.5427927927927928]\n",
      "Время работы lgb_hyperopt_execution_time: 2.9169037342071533\n",
      "Лучшие параметры: {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Лучшая F1-мера: 0.5427927927927928\n",
      "F1 на тестовом наборе данных: 0.5427927927927928\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'n_estimators': hp.choice('n_estimators', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "lgb_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_hyperopt_execution_time:\", lgb_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'max_depth': [3, 4, 5][best['max_depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'n_estimators': [30, 50, 100][best['n_estimators']]\n",
    "}\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_hyperopt_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HiSaVZ4FDASL",
    "outputId": "3473aecb-9833-4f7e-8f3d-9b193c4f6a40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-09 17:04:21,904] A new study created in memory with name: no-name-aca0ddcf-bc75-4cdf-b12e-929e725393c3\n",
      "[I 2023-06-09 17:04:22,375] Trial 0 finished with value: 0.5020527859237536 and parameters: {'max_depth': 4, 'learning_rate': 0.08601226068626279, 'n_estimators': 100}. Best is trial 0 with value: 0.5020527859237536.\n",
      "[I 2023-06-09 17:04:22,641] Trial 1 finished with value: 0.4475172847265871 and parameters: {'max_depth': 4, 'learning_rate': 0.07942700863011141, 'n_estimators': 50}. Best is trial 0 with value: 0.5020527859237536.\n",
      "[I 2023-06-09 17:04:23,165] Trial 2 finished with value: 0.5177304964539008 and parameters: {'max_depth': 5, 'learning_rate': 0.05530366197589997, 'n_estimators': 100}. Best is trial 2 with value: 0.5177304964539008.\n",
      "[I 2023-06-09 17:04:23,373] Trial 3 finished with value: 0.39004707464694016 and parameters: {'max_depth': 4, 'learning_rate': 0.09043196678597118, 'n_estimators': 30}. Best is trial 2 with value: 0.5177304964539008.\n",
      "[I 2023-06-09 17:04:23,728] Trial 4 finished with value: 0.4068466096115866 and parameters: {'max_depth': 3, 'learning_rate': 0.04800379228234639, 'n_estimators': 100}. Best is trial 2 with value: 0.5177304964539008.\n",
      "[I 2023-06-09 17:04:23,913] Trial 5 finished with value: 0.31455742501828826 and parameters: {'max_depth': 3, 'learning_rate': 0.08626575769435064, 'n_estimators': 30}. Best is trial 2 with value: 0.5177304964539008.\n",
      "[I 2023-06-09 17:04:24,154] Trial 6 finished with value: 0.37748798901853126 and parameters: {'max_depth': 3, 'learning_rate': 0.07588198801791139, 'n_estimators': 50}. Best is trial 2 with value: 0.5177304964539008.\n",
      "[I 2023-06-09 17:04:24,604] Trial 7 finished with value: 0.4988262910798123 and parameters: {'max_depth': 4, 'learning_rate': 0.07927117102675643, 'n_estimators': 100}. Best is trial 2 with value: 0.5177304964539008.\n",
      "[I 2023-06-09 17:04:25,044] Trial 8 finished with value: 0.5213872832369942 and parameters: {'max_depth': 4, 'learning_rate': 0.0986104733668862, 'n_estimators': 100}. Best is trial 8 with value: 0.5213872832369942.\n",
      "[I 2023-06-09 17:04:25,305] Trial 9 finished with value: 0.0 and parameters: {'max_depth': 5, 'learning_rate': 0.014652834666886896, 'n_estimators': 30}. Best is trial 8 with value: 0.5213872832369942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время работы lgb_optuna_execution_time: 3.4038119316101074\n",
      "Лучшие параметры: {'max_depth': 4, 'learning_rate': 0.0986104733668862, 'n_estimators': 100}\n",
      "Лучшая F1-мера: 0.5213872832369942\n",
      "F1 на тестовом наборе данных: 0.5213872832369942\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [30, 50, 100])\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "lgb_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_optuna_execution_time:\", lgb_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_optuna_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_optuna_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_HST1STDASL",
    "outputId": "f0f0220b-f790-4eb8-ab72-9f39d03d9eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.4035   \u001b[0m | \u001b[0m0.03808  \u001b[0m | \u001b[0m4.901    \u001b[0m | \u001b[0m81.24    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2888   \u001b[0m | \u001b[0m0.06027  \u001b[0m | \u001b[0m3.312    \u001b[0m | \u001b[0m40.92    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.5023   \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m4.899    \u001b[0m | \u001b[95m79.93    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.5023   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m4.885    \u001b[0m | \u001b[0m79.1     \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.4663   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m79.26    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.2416   \u001b[0m | \u001b[0m0.01988  \u001b[0m | \u001b[0m4.967    \u001b[0m | \u001b[0m75.63    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.4357   \u001b[0m | \u001b[0m0.06061  \u001b[0m | \u001b[0m3.33     \u001b[0m | \u001b[0m99.95    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.336    \u001b[0m | \u001b[0m0.02363  \u001b[0m | \u001b[0m4.757    \u001b[0m | \u001b[0m96.43    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.4181   \u001b[0m | \u001b[0m0.08251  \u001b[0m | \u001b[0m3.054    \u001b[0m | \u001b[0m58.11    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m61.5     \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.443    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m54.58    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.4533   \u001b[0m | \u001b[0m0.09301  \u001b[0m | \u001b[0m4.966    \u001b[0m | \u001b[0m50.72    \u001b[0m |\n",
      "=============================================================\n",
      "Время работы lgb_bayes_opt_execution_time: 9.43139934539795\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 4.899134752996972, 'n_estimators': 79.92904265388121}\n",
      "Лучшая F1-мера: 0.5023474178403756\n",
      "F1 на тестовом наборе данных: 0.5023474178403756\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "lgb_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_bayes_opt_execution_time:\", lgb_bayes_opt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных с лучшими параметрами\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = lgb.LGBMClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "lgb_bayes_opt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_bayes_opt_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2ffJUp-DASL"
   },
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4nLNa6oDASL"
   },
   "source": [
    "Описание алгоритма CatBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель CatBoost с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - CatBoost использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost и LightGBM. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - CatBoost применяет особый подход к кодированию категориальных признаков, называемый симметричным бинарным кодированием, который учитывает взаимодействия между категориями и признаками.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost и LightGBM.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - CatBoost также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost и LightGBM.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - CatBoost поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost и LightGBM.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - CatBoost также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели.\n",
    "   - CatBoost также учитывает веса деревьев на основе их ошибок и значимости, аналогично XGBoost и LightGBM.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели CatBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRmwOfZJDASM"
   },
   "source": [
    "При оптимизации модели CatBoost можно обратить внимание на следующие важные параметры:\n",
    "\n",
    "1. **depth**: Глубина дерева. Этот параметр определяет, насколько глубоким может быть каждое дерево. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр контролирует вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **iterations**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bylevel**: Доля признаков, используемых для построения каждого уровня дерева. Этот параметр контролирует случайную подвыборку признаков для каждого уровня дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **l2_leaf_reg**: L2 регуляризация. Этот параметр помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов.\n",
    "\n",
    "7. **random_strength**: Сила случайности для ускорения обучения. Этот параметр контролирует уровень случайности в выборе признаков при построении дерева. Значение больше 0 добавляет случайность, что может помочь предотвратить переобучение.\n",
    "\n",
    "8. **bagging_temperature**: Температура бэггинга. Этот параметр контролирует степень случайности в выборе объектов для обучения каждого дерева. Значение больше 0 добавляет случайность, что может помочь предотвратить переобучение.\n",
    "\n",
    "Важно отметить, что конкретные параметры и их значения могут варьироваться в зависимости от конкретной задачи и набора данных. Рекомендуется провести эксперименты и настраивать параметры с использованием кросс-валидации для достижения наилучшей производительности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARxiWMrZDASM",
    "outputId": "10433a48-930f-41b1-e8b2-ef56d8784b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время работы catboost_gridsearch_execution_time: 56.67308831214905\n",
      "Лучшие параметры: {'depth': 5, 'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Лучшая F1: 0.5182567322214451\n",
      "F1 на тестовом наборе данных: 0.5148514851485149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение модели CatBoost\n",
    "model = CatBoostClassifier(silent=True)\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "# param_grid = {\n",
    "#     'depth': [3, 4, 5],\n",
    "#     'learning_rate': [0.1, 0.05, 0.01],\n",
    "#     'n_estimators': [30, 50, 100]\n",
    "# }\n",
    "param_grid = {\n",
    "    'depth': [5],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "catboost_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_gridsearch_execution_time:\", catboost_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "catboost_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_gridsearch_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kV4FOtyDDASM",
    "outputId": "9d94b52e-457a-4f9a-de3c-d372bd09f176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/trial, best loss: -0.5148514851485149]\n",
      "Время работы catboost_hyperopt_execution_time: 14.63330340385437\n",
      "Лучшие параметры: {'depth': 5, 'learning_rate': 0.1, 'iterations': 100}\n",
      "Лучшая F1-мера: 0.5148514851485149\n",
      "F1 на тестовом наборе данных: 0.5148514851485149\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'depth': hp.choice('depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'iterations': hp.choice('iterations', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "catboost_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_hyperopt_execution_time:\", catboost_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'depth': [3, 4, 5][best['depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'iterations': [30, 50, 100][best['iterations']]\n",
    "}\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "catboost_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_hyperopt_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlBubWUyDASM",
    "outputId": "451efe28-e71f-4e17-a289-00fc32824e6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-09 17:05:51,364] A new study created in memory with name: no-name-199fe588-80c0-4e6e-b528-9272cf6d0535\n",
      "[I 2023-06-09 17:05:53,344] Trial 0 finished with value: 0.2867981790591806 and parameters: {'depth': 3, 'learning_rate': 0.013007437402729593, 'iterations': 100}. Best is trial 0 with value: 0.2867981790591806.\n",
      "[I 2023-06-09 17:05:54,288] Trial 1 finished with value: 0.2911010558069381 and parameters: {'depth': 4, 'learning_rate': 0.003132738396769772, 'iterations': 30}. Best is trial 1 with value: 0.2911010558069381.\n",
      "[I 2023-06-09 17:05:55,302] Trial 2 finished with value: 0.4251458198314971 and parameters: {'depth': 5, 'learning_rate': 0.08643802891965559, 'iterations': 30}. Best is trial 2 with value: 0.4251458198314971.\n",
      "[I 2023-06-09 17:05:56,373] Trial 3 finished with value: 0.3671232876712328 and parameters: {'depth': 3, 'learning_rate': 0.06498590597136844, 'iterations': 50}. Best is trial 2 with value: 0.4251458198314971.\n",
      "[I 2023-06-09 17:05:57,314] Trial 4 finished with value: 0.30791788856304986 and parameters: {'depth': 3, 'learning_rate': 0.07738223650973663, 'iterations': 30}. Best is trial 2 with value: 0.4251458198314971.\n",
      "[I 2023-06-09 17:05:58,948] Trial 5 finished with value: 0.5029655990510082 and parameters: {'depth': 4, 'learning_rate': 0.09344627089016184, 'iterations': 100}. Best is trial 5 with value: 0.5029655990510082.\n",
      "[I 2023-06-09 17:06:00,182] Trial 6 finished with value: 0.45015673981191223 and parameters: {'depth': 5, 'learning_rate': 0.07491207674434958, 'iterations': 50}. Best is trial 5 with value: 0.5029655990510082.\n",
      "[I 2023-06-09 17:06:01,147] Trial 7 finished with value: 0.2917888563049853 and parameters: {'depth': 3, 'learning_rate': 0.002318157248720638, 'iterations': 30}. Best is trial 5 with value: 0.5029655990510082.\n",
      "[I 2023-06-09 17:06:03,599] Trial 9 finished with value: 0.3234227701232777 and parameters: {'depth': 3, 'learning_rate': 0.06014279651832636, 'iterations': 30}. Best is trial 5 with value: 0.5029655990510082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время работы catboost_hyperopt_execution_time: 12.2300283908844\n",
      "Лучшие параметры: {'depth': 4, 'learning_rate': 0.09344627089016184, 'iterations': 100}\n",
      "Лучшая F1-мера: 0.5029655990510082\n",
      "F1 на тестовом наборе данных: 0.5029655990510082\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'iterations': trial.suggest_categorical('iterations', [30, 50, 100])\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "catboost_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_hyperopt_execution_time:\", catboost_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "catboost_optuna_f1 = f1_score(y_test, model.predict(X_test))\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_optuna_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ9EjWqWDASN",
    "outputId": "a19fbf5f-b9e4-4a93-f26f-8e0af7ae4815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.3813   \u001b[0m | \u001b[0m0.03808  \u001b[0m | \u001b[0m4.901    \u001b[0m | \u001b[0m81.24    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.3152   \u001b[0m | \u001b[0m0.06027  \u001b[0m | \u001b[0m3.312    \u001b[0m | \u001b[0m40.92    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.4875   \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m4.899    \u001b[0m | \u001b[95m79.93    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.4628   \u001b[0m | \u001b[0m0.07988  \u001b[0m | \u001b[0m4.893    \u001b[0m | \u001b[0m79.92    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.4901   \u001b[0m | \u001b[95m0.09815  \u001b[0m | \u001b[95m4.853    \u001b[0m | \u001b[95m80.03    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.4138   \u001b[0m | \u001b[0m0.04491  \u001b[0m | \u001b[0m4.943    \u001b[0m | \u001b[0m80.25    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.4458   \u001b[0m | \u001b[0m0.06957  \u001b[0m | \u001b[0m4.62     \u001b[0m | \u001b[0m79.91    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.3925   \u001b[0m | \u001b[0m0.03552  \u001b[0m | \u001b[0m4.936    \u001b[0m | \u001b[0m80.06    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.383    \u001b[0m | \u001b[0m0.03166  \u001b[0m | \u001b[0m4.748    \u001b[0m | \u001b[0m79.98    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.4613   \u001b[0m | \u001b[0m0.07458  \u001b[0m | \u001b[0m4.562    \u001b[0m | \u001b[0m79.84    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.3579   \u001b[0m | \u001b[0m0.02203  \u001b[0m | \u001b[0m4.473    \u001b[0m | \u001b[0m79.87    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.4371   \u001b[0m | \u001b[0m0.06125  \u001b[0m | \u001b[0m4.562    \u001b[0m | \u001b[0m79.83    \u001b[0m |\n",
      "=============================================================\n",
      "Время работы catboost_bayes_opt_execution_time: 19.0199613571167\n",
      "Лучшие параметры: {'learning_rate': 0.09815464422786867, 'max_depth': 4.85302000157028, 'n_estimators': 80.02730207385395}\n",
      "Лучшая F1-мера: 0.4900662251655629\n",
      "F1 на тестовом наборе данных: 0.4900662251655629\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "catboost_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_bayes_opt_execution_time:\", catboost_bayes_opt_execution_time)\n",
    "\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели с лучшими параметрами на тестовом наборе данных\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = CatBoostClassifier(silent=True, **best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "catboost_bayes_opt_f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_bayes_opt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "J3cyFO6cDASN",
    "outputId": "3e751a6b-a748-436f-ee5d-0137c878f45c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-dd3a9736-e1bf-45f5-b2dd-052acb9cbbf8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель и оптимайзер</th>\n",
       "      <th>F1</th>\n",
       "      <th>Время</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost gridsearch</td>\n",
       "      <td>0.4515731030228254</td>\n",
       "      <td>49.16833782196045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost hyperopt</td>\n",
       "      <td>0.5251141552511416</td>\n",
       "      <td>50.32681059837341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost optuna</td>\n",
       "      <td>0.5210374639769453</td>\n",
       "      <td>43.012019872665405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost bayes_opt</td>\n",
       "      <td>0.5002935995302408</td>\n",
       "      <td>62.50949716567993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM gridsearch</td>\n",
       "      <td>0.5427927927927928</td>\n",
       "      <td>52.43806481361389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM hyperopt</td>\n",
       "      <td>0.5427927927927928</td>\n",
       "      <td>2.9169037342071533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM optuna</td>\n",
       "      <td>0.5213872832369942</td>\n",
       "      <td>3.4038119316101074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBM bayes_opt</td>\n",
       "      <td>0.5023474178403756</td>\n",
       "      <td>9.43139934539795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Catboost gridsearch</td>\n",
       "      <td>0.5148514851485149</td>\n",
       "      <td>56.67308831214905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Catboost hyperopt</td>\n",
       "      <td>0.5148514851485149</td>\n",
       "      <td>14.63330340385437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Catboost optuna</td>\n",
       "      <td>0.5029655990510082</td>\n",
       "      <td>12.2300283908844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Catboost bayes_opt</td>\n",
       "      <td>0.4900662251655629</td>\n",
       "      <td>19.0199613571167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd3a9736-e1bf-45f5-b2dd-052acb9cbbf8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-dd3a9736-e1bf-45f5-b2dd-052acb9cbbf8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-dd3a9736-e1bf-45f5-b2dd-052acb9cbbf8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    Модель и оптимайзер                  F1               Время\n",
       "0    XGBoost gridsearch  0.4515731030228254   49.16833782196045\n",
       "1      XGBoost hyperopt  0.5251141552511416   50.32681059837341\n",
       "2        XGBoost optuna  0.5210374639769453  43.012019872665405\n",
       "3     XGBoost bayes_opt  0.5002935995302408   62.50949716567993\n",
       "4   LightGBM gridsearch  0.5427927927927928   52.43806481361389\n",
       "5     LightGBM hyperopt  0.5427927927927928  2.9169037342071533\n",
       "6       LightGBM optuna  0.5213872832369942  3.4038119316101074\n",
       "7    LightGBM bayes_opt  0.5023474178403756    9.43139934539795\n",
       "8   Catboost gridsearch  0.5148514851485149   56.67308831214905\n",
       "9     Catboost hyperopt  0.5148514851485149   14.63330340385437\n",
       "10      Catboost optuna  0.5029655990510082    12.2300283908844\n",
       "11   Catboost bayes_opt  0.4900662251655629    19.0199613571167"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание данных для таблицы сравнения по f1_score и времени работы\n",
    "params_match = [\n",
    "    ['XGBoost gridsearch', f'{xgb_gridsearch_f1}', f'{xgb_gridsearch_execution_time}'],\n",
    "    ['XGBoost hyperopt', f'{xgb_hyperopt_f1}', f'{xgb_hyperopt_execution_time}'],\n",
    "    ['XGBoost optuna', f'{xgb_optuna_f1}', f'{xgb_optuna_execution_time}'],\n",
    "    ['XGBoost bayes_opt', f'{xgb_bayes_opt_f1}', f'{xgb_bayes_opt_execution_time}'],\n",
    "    ['LightGBM gridsearch', f'{lgb_gridsearch_f1}', f'{lgb_gridsearch_execution_time}'],\n",
    "    ['LightGBM hyperopt', f'{lgb_hyperopt_f1}', f'{lgb_hyperopt_execution_time}'],\n",
    "    ['LightGBM optuna', f'{lgb_optuna_f1}', f'{lgb_optuna_execution_time}'],\n",
    "    ['LightGBM bayes_opt', f'{lgb_bayes_opt_f1}', f'{lgb_bayes_opt_execution_time}'],\n",
    "    ['Catboost gridsearch', f'{catboost_gridsearch_f1}', f'{catboost_gridsearch_execution_time}'],\n",
    "    ['Catboost hyperopt', f'{catboost_hyperopt_f1}', f'{catboost_hyperopt_execution_time}'],\n",
    "    ['Catboost optuna', f'{catboost_optuna_f1}', f'{catboost_optuna_execution_time}'],\n",
    "    ['Catboost bayes_opt', f'{catboost_bayes_opt_f1}', f'{catboost_bayes_opt_execution_time}'],\n",
    "\n",
    "]\n",
    "\n",
    "# Создание DataFrame из данных\n",
    "df = pd.DataFrame(params_match, columns=['Модель и оптимайзер', 'F1', 'Время'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Отображение DataFrame\n",
    "df.head(len(params_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "PxuCOJzkDASN",
    "outputId": "a65a7eb3-fd85-48f6-c0f0-a5bfd1adfe49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Описание параметра</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>CatBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Скорость обучения (Learning Rate)</td>\n",
       "      <td>eta (learning_rate)</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>learning_rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Количество деревьев (Number of Trees)</td>\n",
       "      <td>n_estimators</td>\n",
       "      <td>n_estimators</td>\n",
       "      <td>iterations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Максимальная глубина дерева (Maximum Depth)</td>\n",
       "      <td>max_depth</td>\n",
       "      <td>max_depth</td>\n",
       "      <td>max_depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Минимальный вес листа дерева, Минимальное количество объектов в листе (Minimum Child Weight)</td>\n",
       "      <td>min_child_weight</td>\n",
       "      <td>min_child_samples</td>\n",
       "      <td>min_data_in_leaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Гамма (Gamma)</td>\n",
       "      <td>gamma</td>\n",
       "      <td>min_split_gain</td>\n",
       "      <td>l2_leaf_reg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Доля выборки объектов (Subsample Ratio)</td>\n",
       "      <td>subsample</td>\n",
       "      <td>subsample</td>\n",
       "      <td>subsample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Доля выборки признаков (Column Subsampling Ratio)</td>\n",
       "      <td>colsample_bytree</td>\n",
       "      <td>colsample_bytree</td>\n",
       "      <td>colsample_bylevel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Лямбда (L2-регуляризация)</td>\n",
       "      <td>reg_lambda</td>\n",
       "      <td>reg_lambda</td>\n",
       "      <td>reg_lambda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Альфа (L1-регуляризация)</td>\n",
       "      <td>reg_alpha</td>\n",
       "      <td>reg_alpha</td>\n",
       "      <td>reg_alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Максимальное изменение шага (Maximum Delta Step)</td>\n",
       "      <td>max_delta_step</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Максимальное количество корзин (Maximum Bin Count)</td>\n",
       "      <td>max_bin</td>\n",
       "      <td>max_bin</td>\n",
       "      <td>max_bin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Частота сэмплирования объектов (Sampling Frequency for LGBM)</td>\n",
       "      <td>-</td>\n",
       "      <td>subsample_freq</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Масштабирование веса положительного класса (Scale the Weight of Positive Class)</td>\n",
       "      <td>scale_pos_weight</td>\n",
       "      <td>scale_pos_weight</td>\n",
       "      <td>scale_pos_weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Режим обучения с помощью градиентного спуска (Booster Type)</td>\n",
       "      <td>booster</td>\n",
       "      <td>boosting_type</td>\n",
       "      <td>boosting_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Коэффициент сжатия (Subsample)</td>\n",
       "      <td>subsample</td>\n",
       "      <td>subsample</td>\n",
       "      <td>subsample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Размер выборки для построения дерева (Bagging Fraction)</td>\n",
       "      <td>colsample_bytree</td>\n",
       "      <td>colsample_bytree</td>\n",
       "      <td>colsample_bylevel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Способ выбора ветвления (Split Criterion)</td>\n",
       "      <td>tree_method</td>\n",
       "      <td>tree_learner</td>\n",
       "      <td>grow_policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Функция потерь (Loss Function)</td>\n",
       "      <td>objective</td>\n",
       "      <td>objective</td>\n",
       "      <td>loss_function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Максимальное количество листьев (Max Leaf Nodes)</td>\n",
       "      <td>-</td>\n",
       "      <td>num_leaves</td>\n",
       "      <td>max_leaves</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              Описание параметра  \\\n",
       "0                                                              Скорость обучения (Learning Rate)   \n",
       "1                                                          Количество деревьев (Number of Trees)   \n",
       "2                                                    Максимальная глубина дерева (Maximum Depth)   \n",
       "3   Минимальный вес листа дерева, Минимальное количество объектов в листе (Minimum Child Weight)   \n",
       "4                                                                                  Гамма (Gamma)   \n",
       "5                                                        Доля выборки объектов (Subsample Ratio)   \n",
       "6                                              Доля выборки признаков (Column Subsampling Ratio)   \n",
       "7                                                                      Лямбда (L2-регуляризация)   \n",
       "8                                                                       Альфа (L1-регуляризация)   \n",
       "9                                               Максимальное изменение шага (Maximum Delta Step)   \n",
       "10                                            Максимальное количество корзин (Maximum Bin Count)   \n",
       "11                                  Частота сэмплирования объектов (Sampling Frequency for LGBM)   \n",
       "12               Масштабирование веса положительного класса (Scale the Weight of Positive Class)   \n",
       "13                                   Режим обучения с помощью градиентного спуска (Booster Type)   \n",
       "14                                                                Коэффициент сжатия (Subsample)   \n",
       "15                                       Размер выборки для построения дерева (Bagging Fraction)   \n",
       "16                                                     Способ выбора ветвления (Split Criterion)   \n",
       "17                                                                Функция потерь (Loss Function)   \n",
       "18                                              Максимальное количество листьев (Max Leaf Nodes)   \n",
       "\n",
       "                XGBoost           LightGBM           CatBoost  \n",
       "0   eta (learning_rate)      learning_rate      learning_rate  \n",
       "1          n_estimators       n_estimators         iterations  \n",
       "2             max_depth          max_depth          max_depth  \n",
       "3      min_child_weight  min_child_samples   min_data_in_leaf  \n",
       "4                 gamma     min_split_gain        l2_leaf_reg  \n",
       "5             subsample          subsample          subsample  \n",
       "6      colsample_bytree   colsample_bytree  colsample_bylevel  \n",
       "7            reg_lambda         reg_lambda         reg_lambda  \n",
       "8             reg_alpha          reg_alpha          reg_alpha  \n",
       "9        max_delta_step                  -                  -  \n",
       "10              max_bin            max_bin            max_bin  \n",
       "11                    -     subsample_freq                  -  \n",
       "12     scale_pos_weight   scale_pos_weight   scale_pos_weight  \n",
       "13              booster      boosting_type      boosting_type  \n",
       "14            subsample          subsample          subsample  \n",
       "15     colsample_bytree   colsample_bytree  colsample_bylevel  \n",
       "16          tree_method       tree_learner        grow_policy  \n",
       "17            objective          objective      loss_function  \n",
       "18                    -         num_leaves         max_leaves  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание данных для таблицы\n",
    "params_match = [\n",
    "    ['Скорость обучения (Learning Rate)', 'eta (learning_rate)', 'learning_rate', 'learning_rate'],\n",
    "    ['Количество деревьев (Number of Trees)', 'n_estimators', 'n_estimators', 'iterations'],\n",
    "    ['Максимальная глубина дерева (Maximum Depth)', 'max_depth', 'max_depth', 'max_depth'],\n",
    "    ['Минимальный вес листа дерева, Минимальное количество объектов в листе (Minimum Child Weight)', 'min_child_weight', 'min_child_samples', 'min_data_in_leaf'],\n",
    "    ['Гамма (Gamma)', 'gamma', 'min_split_gain', 'l2_leaf_reg'],\n",
    "    ['Доля выборки объектов (Subsample Ratio)', 'subsample', 'subsample', 'subsample'],\n",
    "    ['Доля выборки признаков (Column Subsampling Ratio)', 'colsample_bytree', 'colsample_bytree', 'colsample_bylevel'],\n",
    "    ['Лямбда (L2-регуляризация)', 'reg_lambda', 'reg_lambda', 'reg_lambda'],\n",
    "    ['Альфа (L1-регуляризация)', 'reg_alpha', 'reg_alpha', 'reg_alpha'],\n",
    "    ['Максимальное изменение шага (Maximum Delta Step)', 'max_delta_step', '-', '-'],\n",
    "    ['Максимальное количество корзин (Maximum Bin Count)', 'max_bin', 'max_bin', 'max_bin'],\n",
    "    ['Частота сэмплирования объектов (Sampling Frequency for LGBM)', '-', 'subsample_freq', '-'],\n",
    "    ['Масштабирование веса положительного класса (Scale the Weight of Positive Class)', 'scale_pos_weight', 'scale_pos_weight', 'scale_pos_weight'],\n",
    "    ['Режим обучения с помощью градиентного спуска (Booster Type)', 'booster', 'boosting_type', 'boosting_type'],\n",
    "    ['Коэффициент сжатия (Subsample)', 'subsample', 'subsample', 'subsample'],\n",
    "    ['Размер выборки для построения дерева (Bagging Fraction)', 'colsample_bytree', 'colsample_bytree', 'colsample_bylevel'],\n",
    "    ['Способ выбора ветвления (Split Criterion)', 'tree_method', 'tree_learner', 'grow_policy'],\n",
    "    ['Функция потерь (Loss Function)', 'objective', 'objective', 'loss_function'],\n",
    "    ['Максимальное количество листьев (Max Leaf Nodes)', '-', 'num_leaves', 'max_leaves']\n",
    "]\n",
    "\n",
    "# Создание DataFrame из данных\n",
    "df = pd.DataFrame(params_match, columns=['Описание параметра', 'XGBoost', 'LightGBM', 'CatBoost'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Отображение DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
