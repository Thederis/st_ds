{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы рассмотрим особенности модели решающего дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h93nb1trtwhu"
   },
   "source": [
    "**Решающее дерево** - это алгоритм машинного обучения, который используется для решения задач классификации и регрессии. Оно представляет собой древовидную структуру, где каждый узел представляет тест на одном из признаков, а каждая ветвь - возможный результат этого теста. Листья дерева представляют собой конечный результат - прогноз для новых данных.\n",
    "\n",
    "В процессе построения решающего дерева, алгоритм выбирает тест, который лучше всего разделяет данные на различные классы или предсказывает значение целевой переменной. Затем данные разбиваются на две или более частей в соответствии с результатами теста. Этот процесс повторяется для каждой полученной части, пока не будет достигнут критерий останова.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Энтропия\n",
    "\n",
    "Энтропийный критерий используется в машинном обучении для измерения неопределенности в наборе данных. Он основан на понятии энтропии из информационной теории.\n",
    "\n",
    "Энтропия является мерой неопределенности. Чем выше энтропия, тем более неопределенным является набор данных. Для бинарной классификации, когда у нас есть два класса, энтропия может быть выражена следующей формулой:\n",
    "\n",
    "$$H(p) = -p_1 \\log_2(p_1) -p_2 \\log_2(p_2)$$\n",
    "\n",
    "где $p_1$ и $p_2$ - вероятности первого и второго классов соответственно.\n",
    "\n",
    "Чтобы применить энтропийный критерий к дереву решений, мы вычисляем энтропию для каждого разбиения на признаке и выбираем разбиение с наименьшей энтропией. Чем меньше энтропия, тем более \"чистым\" является разбиение, то есть чем меньше неопределенности в подмножествах данных, полученных после разбиения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерий Джини\n",
    "\n",
    "Критерий Джини (Gini impurity) также используется в деревьях решений для измерения неопределенности в наборе данных. Он вычисляет вероятность неправильной классификации случайно выбранного элемента, если он случайным образом принадлежит к одному из классов. Для бинарной классификации критерий Джини может быть выражен следующей формулой:\n",
    "\n",
    "$$Gini(p) = 1 - (p_1^2 + p_2^2)$$\n",
    "\n",
    "где $p_1$ и $p_2$ - вероятности первого и второго классов соответственно.\n",
    "\n",
    "Для реализации критерия Джини в дереве решений, аналогично энтропийному критерию, необходимо выполнить следующие шаги:\n",
    "\n",
    "1. Разделить данные по каждому признаку и вычислить критерий Джини для каждого разбиения, используя указанную формулу.\n",
    "\n",
    "2. Вычислить улучшение (impurity reduction) для каждого разбиения. Улучшение показывает, насколько снизилась критерий Джини после разбиения и является разницей между критерием Джини до разбиения и суммой критериев Джини после разбиения.\n",
    "\n",
    "3. Выбрать разбиение с наименьшим критерием Джини или наибольшим улучшением в зависимости от реализации алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации энтропийного критерия в Python для вектора признаков, нам нужно вычислить энтропию для каждого разбиения и выбрать разбиение с наименьшей энтропией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Вычисляет энтропию вектора y со значениями дискретных переменных.\n",
    "    Аргументы:\n",
    "    - y: вектор numpy с дискретными значениями.\n",
    "    Возвращает:\n",
    "    - entropy: значение энтропии типа float.\n",
    "    \"\"\"\n",
    "    # Подсчитываем количество каждого уникального значения в y.\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    # Вычисляем вероятность каждого уникального значения.\n",
    "    probs = counts / len(y)\n",
    "    # Вычисляем значение энтропии.\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "def find_best_split_entropy(X, y):\n",
    "    \"\"\"\n",
    "    Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, используя критерий энтропии.\n",
    "    Аргументы:\n",
    "    - X: вектор numpy с вещественными значениями признаков.\n",
    "    - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "    Возвращает:\n",
    "    - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "    - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "    - best_gain: значение критерия энтропии для лучшего разбиения.\n",
    "    \"\"\"\n",
    "    best_feature, best_threshold, best_gain = None, None, 0\n",
    "    # Итерируемся по всем признакам.\n",
    "    for feature in range(X.shape[1]):\n",
    "        # Находим уникальные значения признака.\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        # Итерируемся по всем возможным пороговым значениям признака.\n",
    "        for threshold in thresholds:\n",
    "            # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
    "            left_indices = X[:, feature] <= threshold\n",
    "            right_indices = X[:, feature] > threshold\n",
    "            # Пропускаем текущую итерацию, если не найдены объекты, которые относятся к левому или правому поддереву.\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                continue\n",
    "            # Определяем вектор целевой переменной для объектов, которые относятся к левому и правому поддереву.\n",
    "            left_y, right_y = y[left_indices], y[right_indices]\n",
    "            # Вычисляем значение критерия энтропии для текущего разбиения.\n",
    "            gain = entropy(y) - (len(left_y) / len(y)) * entropy(left_y) - (len(right_y) / len(y)) * entropy(right_y)\n",
    "            # Обновляем значения лучшего разбиения, если найдено разбиение с большим значением\n",
    "            if gain > best_gain:\n",
    "                best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "    return best_feature, best_threshold, best_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 4], [2, 6], [2, 8], [4, 10], [6, 10], [8, 8], [8, 6], [8, 4], \n",
    "     [6, 6], [6, 4], [10, 2], [6, 8], [7.683182774875691, 2.0378865840698976], [12, 4], [12, 6], [12, 8]])\n",
    "y = np.array(np.array([0]*8 + [1]*8))\n",
    "best_feature, best_threshold, best_gain = find_best_split_entropy(X, y)\n",
    "print(\"Best feature:\", best_feature)\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "print(\"Best gain:\", best_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Изображение](images05/dividing_plane.png \"Название изображения\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерий MSE\n",
    "\n",
    "MSE (Mean Squared Error) используется в машинном обучении для измерения среднеквадратичной ошибки модели. Он является одним из критериев оценки качества модели.\n",
    "\n",
    "MSE вычисляется путем суммирования квадратов разностей между предсказанными значениями модели и фактическими значениями и деления этой суммы на количество наблюдений:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "где $y_i$ - фактическое значение, $\\hat{y_i}$ - предсказанное значение модели, а $n$ - количество наблюдений.\n",
    "\n",
    "Чтобы применить MSE в задаче регрессии, мы минимизируем этот критерий при обучении модели. Меньшее значение MSE указывает на более точные предсказания модели и более близкие к фактическим значениям целевой переменной.\n",
    "\n",
    "MSE (Mean Squared Error) выражается как сумма дисперсии и смещения модели:\n",
    "\n",
    "$\n",
    "\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Error}\n",
    "$\n",
    "\n",
    "где:\n",
    "\n",
    "- Bias (смещение) отражает разницу между средним значением прогнозов модели и истинным средним значением целевой переменной.\n",
    "\n",
    "- Variance (дисперсия) является мерой разброса данных относительно их среднего значения.\n",
    "\n",
    "- Error представляет собой невоспроизводимую случайную составляющую данных.\n",
    "\n",
    "Цель состоит в том, чтобы найти баланс между смещением и дисперсией для достижения наилучшей модели, минимизируя MSE.\n",
    "\n",
    "Таким образом, при использовании MSE в деревьях решений или других моделях регрессии, мы стремимся выбрать разбиения, которые минимизируют среднеквадратичную ошибку, чтобы получить наилучшую модель с наименьшими отклонениями от фактических значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовое дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"Метод требует переопределения в классе наследования\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_split_entropy(X, y):\n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, используя критерий энтропии.\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_gain: значение критерия энтропии для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        def entropy(y):\n",
    "            \"\"\"\n",
    "            Вычисляет энтропию вектора y со значениями дискретных переменных.\n",
    "            Аргументы:\n",
    "            - y: вектор numpy с дискретными значениями.\n",
    "            Возвращает:\n",
    "            - entropy: значение энтропии типа float.\n",
    "            \"\"\"\n",
    "            # Подсчитываем количество каждого уникального значения в y.\n",
    "            _, counts = np.unique(y, return_counts=True)\n",
    "            # Вычисляем вероятность каждого уникального значения.\n",
    "            probs = counts / len(y)\n",
    "            # Вычисляем значение энтропии.\n",
    "            return -np.sum(probs * np.log2(probs))\n",
    "        \n",
    "        best_feature, best_threshold, best_gain = None, None, 0\n",
    "        # Итерируемся по всем признакам.\n",
    "        for feature in range(X.shape[1]):\n",
    "            # Находим уникальные значения признака.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            # Итерируемся по всем возможным пороговым значениям признака.\n",
    "            for threshold in thresholds:\n",
    "                # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                # Пропускаем текущую итерацию, если не найдены объекты, которые относятся к левому или правому поддереву.\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                # Определяем вектор целевой переменной для объектов, которые относятся к левому и правому поддереву.\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                # Вычисляем значение критерия энтропии для текущего разбиения.\n",
    "                gain = entropy(y) - (len(left_y) / len(y)) * entropy(left_y) \\\n",
    "                                       - (len(right_y) / len(y)) * entropy(right_y)\n",
    "                # Обновляем значения лучшего разбиения, если найдено разбиение с большим значением\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "        \n",
    "    @staticmethod   \n",
    "    def find_best_split_mse(X, y):\n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, \n",
    "        используя критерий среднеквадратичной ошибки (MSE).\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_mse: значение критерия среднеквадратичной ошибки для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        best_feature, best_threshold, best_mse = None, None, float('inf')\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                mse = np.mean((left_y - np.mean(left_y))**2) + np.mean((right_y - np.mean(right_y))**2)\n",
    "                if mse < best_mse:\n",
    "                    best_feature, best_threshold, best_mse = feature, threshold, mse\n",
    "        return best_feature, best_threshold, best_mse\n",
    "\n",
    "    def fit(self, X, y, y_pred=None):\n",
    "        \"\"\"\n",
    "        Обучает дерево регрессии на обучающих данных X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "        - y_pred (опционально): вектор numpy с вещественными значениями предсказаний (для Gradient boosting)\n",
    "        \"\"\"\n",
    "        if y_pred is not None:\n",
    "            self.tree = self._build_tree(X, y, depth=0, y_pred=y_pred)\n",
    "        else:\n",
    "            self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Выполняет предсказание для входных данных X.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "\n",
    "        Возвращает:\n",
    "        - predictions: вектор numpy с предсказанными вещественными значениями.\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth, y_pred=None):\n",
    "        \"\"\"\n",
    "        Рекурсивно строит дерево регрессии, используя входные данные X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "        - y_pred (опционально): вектор numpy с вещественными значениями предсказаний (для Gradient boosting)\n",
    "        - depth: текущая глубина дерева.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий узел дерева.\n",
    "\n",
    "        \"\"\"\n",
    "        # Проверка условий останова по максимальной глубине и другим критериям\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            # Создание листового узла\n",
    "            return self._create_leaf_node(y, y_pred)\n",
    "        \n",
    "        # Нахождение лучшего разбиения по критерию энтропии\n",
    "        if self.criterion == \"entropy\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_entropy(X, y)\n",
    "\n",
    "        # Нахождение лучшего разбиения по критерию mse\n",
    "        elif self.criterion == \"mse\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_mse(X, y)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Следует задать критерий разбиения из списка [\"mse\", \"entropy\"]')\n",
    "\n",
    "        # Проверка условия останова, если не удалось найти лучшее разбиение\n",
    "        if best_feature is None or best_threshold is None:\n",
    "            # Создание листового узла\n",
    "            if y_pred is not None:\n",
    "                return self._create_leaf_node(y, y_pred)\n",
    "            return self._create_leaf_node(y)\n",
    "\n",
    "        # Разделение данных на левое и правое поддеревья\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        # Рекурсивное построение левого и правого поддеревьев\n",
    "        if y_pred is not None:\n",
    "            left_tree = self._build_tree(X[left_indices], y[left_indices], y_pred[left_indices], depth + 1)\n",
    "            right_tree = self._build_tree(X[right_indices], y[right_indices], y_pred[right_indices], depth + 1)\n",
    "        else:\n",
    "            left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "            right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        # Создание узла с информацией о лучшем разбиении\n",
    "        node = {'feature': best_feature, 'threshold': best_threshold,\n",
    "                'left': left_tree, 'right': right_tree}\n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred=None):\n",
    "        raise NotImplementedError(\"Метод требует переопределения в классе наследования\")\n",
    "\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Обходит дерево регрессии для выполнения предсказания на входных данных x.\n",
    "\n",
    "        Аргументы:\n",
    "        - x: вектор numpy с вещественными значениями признаков.\n",
    "        - node: текущий узел дерева.\n",
    "\n",
    "        Возвращает:\n",
    "        - value: предсказанное вещественное значение.\n",
    "        \"\"\"\n",
    "        if 'value' in node:\n",
    "            return node['value']\n",
    "\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._traverse_tree(x, node['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, node['right'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дерево для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(BasicTree):\n",
    "    def __init__(self, max_depth=None, criterion=\"mse\"):\n",
    "        \"\"\"\n",
    "        Инициализирует объект RegressionTree.\n",
    "\n",
    "        Аргументы:\n",
    "        - max_depth: максимальная глубина дерева (опционально). \n",
    "        Если значение None, то дерево будет строиться без ограничения глубины.\n",
    "        - criterion: выбор способа разбиений деревьев. Выбирается из списка: \n",
    "        [\"mse\", \"entropy\", \"gini\"]\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred=None):\n",
    "        \"\"\"\n",
    "        Создает листовой узел дерева регрессии.\n",
    "\n",
    "        Аргументы:\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий листовой узел среднего значения целевой переменной.\n",
    "        \"\"\"\n",
    "        return {'value': np.mean(y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y_train = np.array([1, 2, 3, 4])\n",
    "\n",
    "regressor = RegressionTree(max_depth=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array([[2, 3], [6, 7]])\n",
    "y_test = regressor.predict(X_test)\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Визуализация обучающих данных\n",
    "plt.scatter(X_train[:, 0], y_train, color='blue', label='Training Data')\n",
    "\n",
    "# Визуализация предсказанных значений\n",
    "plt.scatter(X_test[:, 0], y_test, color='red', label='Predicted Values')\n",
    "\n",
    "# Добавление линии предсказанных значений\n",
    "plt.plot(X_test[:, 0], y_test, color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y_train = np.array([1, 2, 3, 4])\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array([[2, 3], [6, 7]])\n",
    "y_test = regressor.predict(X_test)\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Визуализация обучающих данных\n",
    "plt.scatter(X_train[:, 0], y_train, color='blue', label='Training Data')\n",
    "\n",
    "# Визуализация предсказанных значений\n",
    "plt.scatter(X_test[:, 0], y_test, color='red', label='Predicted Values')\n",
    "\n",
    "# Добавление линии предсказанных значений\n",
    "plt.plot(X_test[:, 0], y_test, color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(regressor, out_file=None)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graphviz.Source(graph.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дерево для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree(BasicTree):\n",
    "    def __init__(self, max_depth=None, criterion=\"entropy\"):\n",
    "        \"\"\"\n",
    "        Инициализирует объект RegressionTree.\n",
    "\n",
    "        Аргументы:\n",
    "        - max_depth: максимальная глубина дерева (опционально). \n",
    "        Если значение None, то дерево будет строиться без ограничения глубины.\n",
    "        - criterion: выбор способа разбиений деревьев. Выбирается из списка: \n",
    "        [\"mse\", \"entropy\", \"gini\"]\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y):\n",
    "        \"\"\"\n",
    "        Создает листовой узел дерева классификации.\n",
    "\n",
    "        Аргументы:\n",
    "        - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий листовой узел с наиболее часто встречающимся значением целевой переменной.\n",
    "        \"\"\"\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        most_common_class = unique_classes[np.argmax(counts)]\n",
    "        return {'value': most_common_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение дерева классификации\n",
    "X_train = np.array([[1.2, 2.4], [0.5, 1.8], [3.1, 2.9], [2.0, 1.5]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "clf = ClassificationTree(max_depth=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание классов для новых данных\n",
    "X_test = np.array([[1.0, 2.0], [2.5, 1.7]])\n",
    "y_test = clf.predict(X_test)\n",
    "\n",
    "# Вывод предсказанных классов\n",
    "for i, x in enumerate(X_test):\n",
    "    print(f\"Пример {i+1}: {x} -> Класс: {y_test[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X_train, y_train, clf=clf, legend=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "classifier = DecisionTreeClassifier(max_depth=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_test = classifier.predict(X_test)\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(classifier, out_file=None)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graphviz.Source(graph.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X_train, y_train, clf=classifier, legend=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение решающего дерева и линейной модели\n",
    "\n",
    "- Линейные модели могут работать лучше на линейно разделимых данных\n",
    "\n",
    "- При сложных нелинейных зависимостях решающее дерево может работать лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Импорт необходимых библиотек:\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Генерация искусственного набора данных:\n",
    "\"\"\"\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "data = np.zeros(shape=(n, 2))\n",
    "data[:, 0] = np.linspace(-5, 5, n)\n",
    "data[:, 1] = data[:, 0] + 0.5 * np.random.normal(size=n)\n",
    "target = (data[:, 1] > data[:, 0]).astype(int)\n",
    "plt.scatter(data[:, 0], data[:, 1], s=100, c=target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Разделение набора данных на обучающую и тестовую выборки:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Создание объекта линейного классификатора и обучение его на обучающей выборке:\n",
    "\"\"\"\n",
    "linear_classifier = LogisticRegression(random_state=seed)\n",
    "linear_classifier.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Получение прогнозов на тестовой выборке и оценка точности классификатора:\n",
    "\"\"\"\n",
    "y_pred_linear = linear_classifier.predict(X_test)\n",
    "print('f1_score_linear:', f1_score(y_test, y_pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Визуализация разделяющей поверхности линейного классификатора\n",
    "\"\"\"\n",
    "plot_decision_regions(X_test, y_test, linear_classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Создание объекта дерева и обучение его на обучающей выборке:\n",
    "\"\"\"\n",
    "tree_classifier = DecisionTreeClassifier(random_state=seed)\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Получение прогнозов на тестовой выборке и оценка точности дерева:\n",
    "\"\"\"\n",
    "y_pred_tree = tree_classifier.predict(X_test)\n",
    "print('f1_score_tree:', f1_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Визуализация разделяющей поверхности дерева\n",
    "\"\"\"\n",
    "plot_decision_regions(X_test, y_test, tree_classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бэггинг для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BaggingRegressor:\n",
    "    def __init__(self, n_estimators=10, max_depth=None):\n",
    "        \"\"\"\n",
    "        Конструктор класса BaggingRegressor.\n",
    "\n",
    "        Аргументы:\n",
    "        - n_estimators: количество базовых моделей (деревьев).\n",
    "        - max_depth: максимальная глубина деревьев.\n",
    "\n",
    "        Переменные экземпляра:\n",
    "        - n_estimators: количество базовых моделей (деревьев).\n",
    "        - max_depth: максимальная глубина деревьев.\n",
    "        - estimators: список базовых моделей (деревьев).\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель на данных X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Создание и обучение экземпляра RegressionTree\n",
    "            estimator = RegressionTree(max_depth=self.max_depth)\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_sampled = X[indices]\n",
    "            y_sampled = y[indices]\n",
    "            estimator.fit(X_sampled, y_sampled)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Прогнозирует значения для новых данных X.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "\n",
    "        Возвращает:\n",
    "        - predictions: вектор numpy с прогнозируемыми значениями.\n",
    "        \"\"\"\n",
    "        predictions = np.zeros(len(X))\n",
    "        for estimator in self.estimators:\n",
    "            predictions += estimator.predict(X)\n",
    "        return predictions / len(self.estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y_train = np.array([1, 2, 3, 4])\n",
    "\n",
    "bagging_regressor = BaggingRegressor(n_estimators=5, max_depth=3)\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array([[2, 3], [6, 7]])\n",
    "y_test = bagging_regressor.predict(X_test)\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Визуализация обучающих данных\n",
    "plt.scatter(X_train[:, 0], y_train, color='blue', label='Training Data')\n",
    "\n",
    "# Визуализация предсказанных значений\n",
    "plt.scatter(X_test[:, 0], y_test, color='red', label='Predicted Values')\n",
    "\n",
    "# Добавление линии предсказанных значений\n",
    "plt.plot(X_test[:, 0], y_test, color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Определение базового оценщика (решающего дерева регрессии)\n",
    "base_estimator = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Определение бэггинг-регрессора\n",
    "bagging = BaggingRegressor(estimator=base_estimator, n_estimators=5, random_state=42)\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "y_test = bagging.predict(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Визуализация обучающих данных\n",
    "plt.scatter(X_train[:, 0], y_train, color='blue', label='Training Data')\n",
    "\n",
    "# Визуализация предсказанных значений\n",
    "plt.scatter(X_test[:, 0], y_test, color='red', label='Predicted Values')\n",
    "\n",
    "# Добавление линии предсказанных значений\n",
    "plt.plot(X_test[:, 0], y_test, color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бэггинг для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(self, n_estimators=10, max_depth=None):\n",
    "        \"\"\"\n",
    "        Конструктор класса BaggingClassifier.\n",
    "\n",
    "        Аргументы:\n",
    "        - n_estimators: количество базовых моделей (деревьев).\n",
    "        - max_depth: максимальная глубина деревьев.\n",
    "\n",
    "        Переменные экземпляра:\n",
    "        - n_estimators: количество базовых моделей (деревьев).\n",
    "        - max_depth: максимальная глубина деревьев.\n",
    "        - estimators: список базовых моделей (деревьев).\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель на данных X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Создание и обучение экземпляра ClassificationTree\n",
    "            estimator = ClassificationTree(max_depth=self.max_depth)\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_sampled = X[indices]\n",
    "            y_sampled = y[indices]\n",
    "            estimator.fit(X_sampled, y_sampled)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Прогнозирует значения классов для новых данных X.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "\n",
    "        Возвращает:\n",
    "        - predictions: вектор numpy с прогнозируемыми значениями классов.\n",
    "        \"\"\"\n",
    "        predictions = np.zeros(len(X))\n",
    "        for estimator in self.estimators:\n",
    "            predictions += estimator.predict(X)\n",
    "        return np.round(predictions / len(self.estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение бэггинга для классификации\n",
    "X_train = np.array([[1.2, 2.4], [0.5, 1.8], [3.1, 2.9], [2.0, 1.5]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "bagging_model = BaggingClassifier(n_estimators=5, max_depth=3)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание классов для новых данных\n",
    "X_test = np.array([[1.0, 2.0], [2.5, 1.7]])\n",
    "y_test = bagging_model.predict(X_test)\n",
    "\n",
    "# Вывод предсказанных классов\n",
    "for i, x in enumerate(X_test):\n",
    "    print(f\"Пример {i+1}: {x} -> Класс: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X_train, y_train, clf=bagging_model, legend=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Создание и обучение бэггинга для классификации\n",
    "X_train = np.array([[1.2, 2.4], [0.5, 1.8], [3.1, 2.9], [2.0, 1.5]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Создание и обучение BaggingClassifier с базовой моделью DecisionTreeClassifier\n",
    "base_model = DecisionTreeClassifier(max_depth=3)\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование классов на тестовой выборке\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Предсказание классов для новых данных\n",
    "X_test = np.array([[1.0, 2.0], [2.5, 1.7]])\n",
    "y_test = bagging_model.predict(X_test)\n",
    "\n",
    "# Вывод предсказанных классов\n",
    "for i, x in enumerate(X_test):\n",
    "    print(f\"Пример {i+1}: {x} -> Класс: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X_train, y_train, clf=bagging_model, legend=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y_train = np.array([1, 2, 3, 4])\n",
    "\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "# Создание и обучение модели случайного леса\n",
    "model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array([[2, 3], [6, 7]])\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "# Прогнозирование на тестовом наборе данных\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Визуализация обучающих данных\n",
    "plt.scatter(X_train[:, 0], y_train, color='blue', label='Training Data')\n",
    "\n",
    "# Визуализация предсказанных значений\n",
    "plt.scatter(X_test[:, 0], y_test, color='red', label='Predicted Values')\n",
    "\n",
    "# Добавление линии предсказанных значений\n",
    "plt.plot(X_test[:, 0], y_test, color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Создание и обучение случайного леса для классификации\n",
    "X_train = np.array([[1.2, 2.4], [0.5, 1.8], [3.1, 2.9], [2.0, 1.5]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Создание и обучение модели случайного леса\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование классов для тестовых данных\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Предсказание классов для новых данных\n",
    "X_test = np.array([[1.0, 2.0], [2.5, 1.7]])\n",
    "y_test = clf.predict(X_test)\n",
    "\n",
    "# Вывод предсказанных классов\n",
    "for i, x in enumerate(X_test):\n",
    "    print(f\"Пример {i+1}: {x} -> Класс: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X_train, y_train, clf=clf, legend=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Концепция \"out-of-bag\"**\n",
    "\n",
    "Концепция \"out-of-bag\" (OOB) оценки является частью алгоритма случайного леса (random forest) и позволяет оценивать производительность модели без необходимости в отдельном наборе данных для валидации.\n",
    "\n",
    "При построении каждого дерева в случайном лесу, используется только подмножество обучающих данных, которое выбирается случайным образом с возвращением. Это означает, что некоторые образцы данных не попадают в выборку для обучения данного дерева.\n",
    "\n",
    "OOB-оценка использует эти образцы, которые не были включены в обучение конкретного дерева, для оценки его производительности. Каждый образец может быть классифицирован с использованием всех деревьев, которые не включали этот образец в свою обучающую выборку.\n",
    "\n",
    "Процесс оценки out-of-bag состоит из следующих шагов:\n",
    "1. Для каждого образца данных из обучающей выборки, который не был использован для обучения конкретного дерева, происходит классификация с использованием всех деревьев, которые не содержали этот образец в своей обучающей выборке.\n",
    "2. Прогнозы каждого дерева усредняются или голосуют для получения итогового прогноза для данного образца.\n",
    "3. Используя истинные метки классов для образцов, которые не были использованы для обучения дерева, оценивается производительность дерева путем сравнения его прогнозов с истинными метками классов.\n",
    "\n",
    "Итоговая оценка out-of-bag является усредненной оценкой производительности каждого дерева на его соответствующем наборе out-of-bag образцов. Эта оценка может служить в качестве приближенной оценки производительности модели на новых данных, поскольку она использует образцы, которые не были включены в обучение каждого отдельного дерева.\n",
    "\n",
    "OOB-оценка предоставляет удобный способ оценки модели без необходимости валидационного набора данных и может быть полезна для выбора оптимального числа деревьев в случайном лесу или для сравнения различных моделей случайного леса.\n",
    "\n",
    "Оценка out-of-bag точности не всегда является точной оценкой производительности модели и может быть несколько смещенной. Для получения более точной оценки производительности модели рекомендуется использовать кросс-валидацию или отдельный набор данных для валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Генерация датасета\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Инициализация модели\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "\n",
    "# Обучение модели\n",
    "rf.fit(X, y)\n",
    "\n",
    "# out-of-bag (accuracy)\n",
    "oob_score = rf.oob_score_\n",
    "print(\"Out-of-Bag Score:\", oob_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Wrangling & EDA with Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
