{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Модуль 3. Мультимодальные и мультизадачные модели. Часть 1.\n",
        "##Семинар №5. Техники Fusion\n",
        "##Цель занятия:\n",
        "Рассмотреть задачу, где с помощью fusion модальностей можно улучшить качество модели\n",
        "\n",
        "##Подготовительный этап:\n",
        "\n",
        "Загружаем данные для семинара. Вам нужно получить kaggle API ключ для того, чтобы загрузить данные.\n",
        "\n",
        "Гайд по созданию API ключ: https://www.kaggle.com/docs/api"
      ],
      "metadata": {
        "id": "KxIO8uU_N0lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "with open('kaggle.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  os.environ['KAGGLE_USERNAME'] = data[\"username\"]\n",
        "  os.environ['KAGGLE_KEY'] = data[\"key\"]"
      ],
      "metadata": {
        "id": "hNvvCNG-N0xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "id": "Y1p5x4ARo54C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c rsna-miccai-brain-tumor-radiogenomic-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwty8f3__eBZ",
        "outputId": "6b6a25d3-235c-435b-dcfa-8e14b471f2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading rsna-miccai-brain-tumor-radiogenomic-classification.zip to /content\n",
            "100% 12.3G/12.3G [01:58<00:00, 31.2MB/s]\n",
            "100% 12.3G/12.3G [01:58<00:00, 111MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -l /content/rsna-miccai-brain-tumor-radiogenomic-classification.zip"
      ],
      "metadata": {
        "id": "d6EVBym2e2zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -p /content/rsna-miccai-brain-tumor-radiogenomic-classification.zip train_labels.csv > train_labels.csv"
      ],
      "metadata": {
        "id": "PmSf3t2QeKZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/rsna-miccai-brain-tumor-radiogenomic-classification.zip\",\"r\") as zip_ref:\n",
        "    list_of_files = zip_ref.namelist()"
      ],
      "metadata": {
        "id": "viSy_40Ag7gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/rsna-miccai-brain-tumor-radiogenomic-classification.zip\",\"r\") as zip_ref:\n",
        "    for n in range(101):\n",
        "        x = f'train/{n:05}'\n",
        "        res = list(filter(lambda y: y.startswith(x), list_of_files))\n",
        "        for elem in res:\n",
        "            zip_ref.extract(elem, 'folder')"
      ],
      "metadata": {
        "id": "l45uE0mjeIG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данные готовы!"
      ],
      "metadata": {
        "id": "zDCfbA9xOEat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Мультимодальные модели глубокого обучения\n",
        "\n"
      ],
      "metadata": {
        "id": "jqXKKSK651jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Подобно врачам-людям, автоматизированные системы обнаружения и классификации, которые используют как данные медицинской визуализации, так и клинические данные из ЭМК, такие как демографические данные пациентов, предыдущие диагнозы и лабораторные данные, вероятно, создадут модели с более высокой производительностью. Недавняя литература по медицинской визуализации также демонстрирует аналогичную тенденцию, когда как EHR, так и пиксельные данные используются в «парадигме слияния» для решения сложных задач, которые невозможно легко решить с помощью одного лишь метода (Huang et al. 2020)."
      ],
      "metadata": {
        "id": "zlFXADQM52Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Современная медицина опирается на синтез широкого спектра информации, такой как данные визуализации, лабораторные данные, неструктурированные повествовательные данные, 1D-сигналы, такие как ЭКГ, а в некоторых случаях аудиоданные или данные наблюдений. Клинический контекст изображений часто играет решающую роль в принятии диагностических решений при интерпретации медицинских изображений. Например, неоднократно было показано, что отсутствие доступа к клиническим и лабораторным данным во время интерпретации изображений приводит к снижению производительности и снижению клинической полезности для направляющего специалиста. В опросе рентгенологов большинство (87%) заявили, что клиническая информация оказывает значительное влияние на интерпретацию (Boonn and Langlotz 2009). Радиология — не единственная медицинская специальность, основанная на визуализации, которая полагается на контекст для точной интерпретации данных визуализации; патология, офтальмология и дерматология также используют клинический контекст для интерпретации клинических изображений. Имея точную и актуальную информацию о текущих симптомах и истории болезни, врачи могут лучше интерпретировать результаты визуализации в соответствующем клиническом контексте, что приводит к более точному дифференциальному диагнозу, более полезному отчету для врачей и более благоприятному исходу для пациента.\n",
        "####Модели сверточных нейронных сетей (CNN) обычно используют значения пикселей в качестве входных данных без контекстуализации другой клинической информации, как это делают врачи в клинической практике, что может ограничивать производительность. В качестве примера рассмотрим «простую» задачу в радиологии по выявлению пневмонии на рентгенограмме грудной клетки (CXR), которую удалось решить многим исследователям, обучающим модели глубокого обучения для автоматического обнаружения и классификации патологий. Однако такие приложения в конечном итоге могут иметь ограниченное влияние на клиническую практику без клинического контекста, такого как основная жалоба, история настоящего заболевания, история болезни в прошлом, история приема лекарств, семейный анамнез, результаты медицинского осмотра и лабораторные показатели. Несмотря на наличие результатов визуализации, которые могут отличить пневмонию от других диагнозов, рентгенограмма неспецифична, и для точного диагноза требуется клиническая и лабораторная информация. Результаты рентгенологической рентгенографии, предполагающие пневмонию, будут точными у пациентов с лихорадкой и повышенным количеством лейкоцитов, но у пациентов без подобных клинических особенностей или лабораторных показателей подобные изображения могут указывать на ателектаз, отек легких, интерстициальное заболевание или даже рак легких.\n",
        "\n",
        "####Сначала мы обсудим различные методы слияния в глубоком и машинном обучении, затем обсудим некоторые медицинские примеры мультимодальности и, наконец, разработаем простую мультимодальную модель."
      ],
      "metadata": {
        "id": "sDgVplot52Jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Способы слияния"
      ],
      "metadata": {
        "id": "_xqo7snH52ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Слияние изображений — это метод объединения информации из нескольких источников данных изображений, полученных с помощью одних и тех же или разных модальностей. Целью объединения изображений, особенно в медицинской визуализации, является улучшение или дополнение функций каждого источника данных, чтобы алгоритмы машинного обучения могли достичь более высокой производительности, а не использовать только один канал данных. Ожидаемый результат будет иметь более богатое представление функций, чем отдельные компоненты. Техника мультимодального слияния добилась замечательных успехов в различных приложениях, таких как сегментация медицинских изображений, классификация изображений и реконструкция изображений.\n",
        "####Для объединения данных требуется более одного источника данных (например, это могут быть данные одного исследования, например Т1 и Т2 МРТ, или разных исследований, таких как МРТ и КТ). В традиционной литературе по обработке изображений они называются «каналами», и мы также будем использовать эту номенклатуру. Каналы данных могут быть разнородными, дополняющими друг друга, согласованными или несогласованными, синхронными или асинхронными, а также избыточными с разными масштабами, которые необходимо нормализовать для объединения. Медицинские изображения, такие как МРТ/КТ/ПЭТ/УЗИ, также имеют различия, которые необходимо учитывать при их совместном использовании. Поэтому выбор алгоритма или архитектуры зависит в первую очередь от цели приложения и характеристик канала передачи данных. Подводя итог, преимущества объединения данных (Bellot et al. 2002):\n",
        "####1) Предоставить различную информацию, которая поможет повысить уверенность в диагнозе.\n",
        "####2) Чтобы объединить исходные функции и извлечь новую или скрытую информацию с повышенной достоверностью.\n",
        "####3) Повысить абстракцию и полноту данных за счет их эффективного объединения.\n",
        "####В недавних публикациях по компьютерному зрению люди исследуют стратегии мультимодального слияния в контексте пространственно-временной сверточной нейронной сети (Karpathy et al. 2014) и в целом разделяют их на четыре модели/класса слияния: раннее слияние, позднее слияние, совместное слияние и медленное слияние."
      ],
      "metadata": {
        "id": "8mMAMFEoA0lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Раннее слияние:"
      ],
      "metadata": {
        "id": "TnNx8lo-A_H5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Раннее слияние объединяет различные каналы данных в начале обучения модели машинного обучения. Эти отдельные особенности модальности могут быть объединены различными способами, например, с помощью пулинга по среднему значению, взвешенной конкатенации или методов контролируемого слияния (тип I или II на рис. 1). Унимодальные локальные объекты первоначально извлекаются из отдельных входных данных, а затем объединяются в совместное представление. Следовательно, унифицированная модель должна гарантировать, что данные/функции правильно выровнены, чтобы обеспечить совместную обработку. В результате преобразование источников данных в единый вектор признаков является серьезной проблемой при использовании ранних подходов к слиянию данных. В действительности, условно независимые критерии между источниками медицинских данных не всегда верны, поскольку несколько модальностей могут иметь сильно коррелирующие характеристики, возможно, из-за условий сбора данных. Таким образом, различные способы сбора данных обычно пространственно выровнены (так называемая регистрация изображений) со стандартной частотой сэмлпирования пикселей (для изображений или, возможно, временем для других модальностей), чтобы создать общую основу для объединения. Если данные выровнены правильно, можно использовать взаимную корреляцию между элементами данных, что дает возможность повысить производительность системы (Gadzicki et al. 2020).\n",
        "\n",
        "<img src=\"https://i.ibb.co/WchcPN1/fig1.png\"><br>\n",
        "*Figure 1.* **Early Fusion**<br><br>\n"
      ],
      "metadata": {
        "id": "rqz26lcnBCHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Позднее или ансамблевое слияние:"
      ],
      "metadata": {
        "id": "dlcrWILLA_ZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####При позднем слиянии отдельные потоки полностью обрабатывают каждый канал данных отдельно, а их выходные данные объединяются на этапе принятия решения или прогнозирования посредством суммирования, операции усреднения или голосования большинством. Подобно методу ансамбля, процесс позднего слияния обучается оптимальному комбинированию каждого компонента, которое максимизирует производительность. Процесс обучения обрабатывает ошибки независимо, декоррелирует ошибки и, таким образом, повышает производительность в отношении каждой модальности. Главный недостаток заключается в ограниченных возможностях использования взаимной корреляции между различными унимодальными данными.\n",
        "<img src=\"https://i.ibb.co/T2JHW4X/fig2.png\"><br>\n",
        "*Рисунок 2.* **Поздний слияние** <br><br>\n",
        "###Совместное слияние\n",
        "####Совместное слияние было введено для объединения представлений функций визуализации с клиническими особенностями перед их подачей в модель. Из-за различий между визуализацией и клиническими особенностями в размерности и динамическом диапазоне были представлены разные модели (Haylat 2020) для масштабирования их клинических характеристик перед слиянием и улучшения их характеристик (рис. 3).\n",
        "<img src=\"https://i.ibb.co/kKxVYfz/fig3.png\"><br>\n",
        "*Рисунок 3.* **Совместное слияние, тип I (вверху) и тип II (внизу)** <br><br>\n",
        "###Медленный синтез\n",
        "####Медленное слияние широко используется в 3D-видео, как пространственное, так и временное измерения (Хуанг и др., 2020) начинаются с нескольких сетей, которые принимают последовательные видеокадры и медленно объединяют их временные характеристики по всей сети, так что более высокие уровни постепенно получают доступ к глобальной информации (Karpathy et al. 2014). Как показано на рис. 4, вся архитектура разделена на четыре уровня действий, которые постепенно разделяют параметры обучения. На первом уровне каждая модель использует четыре временные последовательности, которые проходят через серию сверток, создавая четыре карты признаков. Второй уровень обрабатывает серию из двух временных признаков из четырех карт признаков и передает их на третий уровень, последовательно получая информацию по всем входным видеокадрам. Наконец, четвертый уровень выполняет операцию пулинга для глобального извлечения признаков и в конечном итоге соединяет более высокие задачи (классификацию) плотным соединением. Таким образом, модели машинного обучения или CNN извлекают выгоду из изучения мощных функций с последующими совмещенными связями и могут быть устойчивыми к деталям связности во времени.\n",
        "<img src=\"https://i.ibb.co/St6Ccz9/fig4.png\"><br>\n",
        "*Рисунок 4.* **Медленное слияние.**\n",
        "\n"
      ],
      "metadata": {
        "id": "D5T3ICSjBQDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Мультимодальное обучение с использованием только изображений:\n",
        "\n",
        "####Как мы обсуждали ранее, мультимодальность может возникать из множества источников; здесь обсуждается обучение «изображение-изображение».\n",
        "\n",
        "####Мультимодальные визуализирующие исследования обычно требуют совместной регистрации. Процесс совместной регистрации изображений включает геометрическое выравнивание двух или более изображений так, чтобы их соответствующие пиксели (вокселы) представляли один и тот же объект. В целом совместную регистрацию изображений можно разделить на две категории в зависимости от ссылки: при первом подходе изображения регистрируются в существующем атласе; другой подход регистрирует изображения в одно выбранное изображение. Очень важно, чтобы этот процесс был предпринят до любого последующего количественного анализа изображения. Для регистрации изображений доступны различные методы, включая регистрацию на основе DL и традиционную регистрацию, которые выходят за рамки этой главы (для дальнейшего чтения см. (Haskins, Kruger and Yan 2020)).\n",
        "####Мультимодальная визуализация имеет множество реальных примеров, одним из которых является сочетание структурной и функциональной (молекулярной) визуализации, такой как ПЭТ-КТ и ПЭТ-МР. Поскольку молекулярная визуализация имеет ограниченное пространственное разрешение, она не может точно показать расположение опухолей, а индикаторы часто не захватываются достаточным количеством структур, чтобы прояснить анатомическое расположение. Получая изображения КТ или МРТ на одном физическом устройстве, можно определить точное анатомическое расположение (рис. 5).\n",
        "<img src=\"https://i.ibb.co/vZf4V1b/fig5.png\"><br>\n",
        "*Рисунок 5.* **(a) Совместно зарегистрированная ПЭТ-КТ, (b) КТ, (c) ПЭТ**<br><br>\n",
        "####Еще один пример — слияние КТ-МРТ для лучевой терапии. МРТ имеет превосходное разрешение мягких тканей и может выявить опухоль, которую трудно оценить на КТ. С другой стороны, МРТ не отражает ослабление радиации, что имеет решающее значение для точной доставки дозы. В результате объединение изображений МРТ с КТ широко используется в радиохирургии, интервенционной радиологии и лучевой терапии. (Лю и др., 2019) (рис. 6)\n",
        "<img src=\"https://i.ibb.co/qjyf2Z6/fig6.png\"><br>\n",
        "*Рисунок 6.* **(а) КТ, (б) зарегистрированная КТ-МРТ, (в) МРТ **<br><br>\n",
        "####Хотя в приведенных выше примерах мы использовали разные модальности, бывают случаи, когда плоскости или способы получения одной и той же модальности могут дать разные типы информации, которую можно использовать. Например, использование как медиолатеральной косой (MLO), так и краниокаудальной (CC) проекций маммограммы повышает точность диагностической визуализации для выявления рака молочной железы по сравнению с использованием только одной. (рис. 7)"
      ],
      "metadata": {
        "id": "Zywf7lReBqPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Здесь мы собираемся обучить модель определению статуса метилирования промотора MGMT у пациентов с опухолью ГБМ.\n",
        "##### Мы собираемся использовать T2, T1, постконтрастные изображения T1, последовательности МРТ FLAIR, а также маску опухоли."
      ],
      "metadata": {
        "id": "4Z2etLhACNAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загружаем библиотеки"
      ],
      "metadata": {
        "id": "8PicDltp52P6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCaQhcI35V9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da73be26-1239-457a-85c5-85e6e5fec680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install monai wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import monai as mn\n",
        "import nibabel as nib\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import wandb"
      ],
      "metadata": {
        "id": "uyocR6uM6FWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выбираем один GPU, если их много / Настраиваем среду WANDB"
      ],
      "metadata": {
        "id": "yT3tbMVq6UOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
        "os.environ['WANDB_API_KEY']= #Введите здесь ваш WAND API KEY.\n",
        "os.environ['WANDB_SILENT']='true'"
      ],
      "metadata": {
        "id": "PJtiP0g36aWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Делаем процесс обучения детерминированным"
      ],
      "metadata": {
        "id": "FeNhAD-j6kv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    mn.utils.misc.set_determinism(seed=seed)\n",
        "\n",
        "\n",
        "seed_all(123)"
      ],
      "metadata": {
        "id": "4dld6ZwH6s2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Настраиваем некоторые гиперпараметры"
      ],
      "metadata": {
        "id": "7MVDJmzm6u2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гиперпараметры\n",
        "bs = 4\n",
        "lr = 1e-5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 15"
      ],
      "metadata": {
        "id": "CeZJf0yp6vCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загружаем метки (статус метилирования MGMT) для каждого пациента"
      ],
      "metadata": {
        "id": "OvdfD61f682M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/train_labels.csv\")  # csv путь\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "-lbvq3YI73CJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "5b693e44-df70-44de-be03-0906e8861d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   BraTS21ID  MGMT_value\n",
              "0          0           1\n",
              "1          2           1\n",
              "2          3           0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64880231-1b98-4ca4-8543-3aeefaba91e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BraTS21ID</th>\n",
              "      <th>MGMT_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64880231-1b98-4ca4-8543-3aeefaba91e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-64880231-1b98-4ca4-8543-3aeefaba91e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-64880231-1b98-4ca4-8543-3aeefaba91e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-faab4ad2-51f6-4045-8fa7-81ebc88589bc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-faab4ad2-51f6-4045-8fa7-81ebc88589bc')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-faab4ad2-51f6-4045-8fa7-81ebc88589bc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_0 = os.listdir(\"/content/folder/train\")"
      ],
      "metadata": {
        "id": "XJ8A6OhE8RSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def path_maker_list(patient_list: list, path: str):\n",
        "    path_list = []\n",
        "    for item in patient_list:\n",
        "        path_patient = os.path.join(path, item)\n",
        "        path_list.append(path_patient)\n",
        "    return path_list"
      ],
      "metadata": {
        "id": "fHnFPQwQ8RV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_list_0 = path_maker_list(\n",
        "    list_0, \"/content/folder/train\"\n",
        ")\n",
        "len(path_list_0)"
      ],
      "metadata": {
        "id": "gEONGnkE8RY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbc79b1-efee-4859-b5f5-802209dc747c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Создайте список словарей, которые для каждого пациента содержат пути к различным последовательностям МРТ и метки."
      ],
      "metadata": {
        "id": "tKiy-7rr8lNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[:65]"
      ],
      "metadata": {
        "id": "w3gSbINUsmgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Разделение датасета для обучения и валидации на уровне пациентов"
      ],
      "metadata": {
        "id": "ykf3uuYV7zpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(\n",
        "    df,\n",
        "    test_size=0.19,\n",
        "    train_size=0.81,\n",
        "    random_state=123,\n",
        "    shuffle=True,\n",
        "    stratify=df[\"MGMT_value\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Ets_SqDe7--Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of positive train samples:\", len(df_train[df_train[\"MGMT_value\"] == 1]))\n",
        "print(\"Number of negative train samples:\", len(df_train[df_train[\"MGMT_value\"] == 0]))\n",
        "print(\"Number of positive validation samples:\", len(df_val[df_val[\"MGMT_value\"] == 1]))\n",
        "print(\"Number of negative validation samples:\", len(df_val[df_val[\"MGMT_value\"] == 0]))\n",
        "print(\n",
        "    \"Ratio of positive to negative samples in train set:\",\n",
        "    len(df_train[df_train[\"MGMT_value\"] == 1])\n",
        "    / len(df_train[df_train[\"MGMT_value\"] == 0]),\n",
        ")\n",
        "print(\n",
        "    \"Ratio of positive to negative samples in validation set:\",\n",
        "    len(df_val[df_val[\"MGMT_value\"] == 1]) / len(df_val[df_val[\"MGMT_value\"] == 0]),\n",
        ")"
      ],
      "metadata": {
        "id": "Zv-UlMmS8BEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cbb55c-a9d8-47a3-ad09-2ec46cbeca76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive train samples: 32\n",
            "Number of negative train samples: 20\n",
            "Number of positive validation samples: 8\n",
            "Number of negative validation samples: 5\n",
            "Ratio of positive to negative samples in train set: 1.6\n",
            "Ratio of positive to negative samples in validation set: 1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"MGMT_value\"].hist(grid=False, bins=3)\n",
        "len_neg = len(df[df[\"MGMT_value\"] == 0])\n",
        "len_pos = len(df[df[\"MGMT_value\"] == 1])\n",
        "print(\n",
        "    f\"Number of Total positive samples:{len_pos}\\nNumber of Total Negative samples:{len_neg}\"\n",
        ")"
      ],
      "metadata": {
        "id": "h5dW5Ox08D-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "be39709d-542f-473d-be11-b92b56b75c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Total positive samples:40\n",
            "Number of Total Negative samples:25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import pydicom\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import glob\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm import tqdm\n",
        "import re"
      ],
      "metadata": {
        "id": "D99pQY-DmyRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создаем датасет"
      ],
      "metadata": {
        "id": "Sy7bJGaQmTbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_IMAGES_3D = 64\n",
        "TRAINING_BATCH_SIZE = 8\n",
        "TEST_BATCH_SIZE = 8\n",
        "IMAGE_SIZE = 256\n",
        "N_EPOCHS = 15\n",
        "do_valid = True\n",
        "n_workers = 4\n",
        "type_ = \"T1wCE\""
      ],
      "metadata": {
        "id": "8_wBc58MmVE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dicom_image(path, img_size=IMAGE_SIZE, voi_lut=True, rotate=0):\n",
        "    dicom = pydicom.read_file(path)\n",
        "    data = dicom.pixel_array\n",
        "    if voi_lut:\n",
        "        data = apply_voi_lut(dicom.pixel_array, dicom)\n",
        "    else:\n",
        "        data = dicom.pixel_array\n",
        "\n",
        "    if rotate > 0:\n",
        "        rot_choices = [\n",
        "            0,\n",
        "            cv2.ROTATE_90_CLOCKWISE,\n",
        "            cv2.ROTATE_90_COUNTERCLOCKWISE,\n",
        "            cv2.ROTATE_180,\n",
        "        ]\n",
        "        data = cv2.rotate(data, rot_choices[rotate])\n",
        "\n",
        "    data = cv2.resize(data, (img_size, img_size))\n",
        "    return data\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class BrainRSNADataset(Dataset):\n",
        "    def __init__(\n",
        "        self, data, transform=None, target=\"MGMT_value\", mri_type=\"FLAIR\", is_train=True\n",
        "    ):\n",
        "        self.target = target\n",
        "        self.data = data\n",
        "        self.type = mri_type\n",
        "\n",
        "        self.transform = transform\n",
        "        self.is_train = is_train\n",
        "        self.folder = \"train\" if self.is_train else \"test\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        case_id = int(row.BraTS21ID)\n",
        "        target = int(row[self.target])\n",
        "        _3d_images = self.load_dicom_images_3d(case_id, self.type)\n",
        "        _3d_images = torch.tensor(_3d_images).float()\n",
        "        if self.is_train:\n",
        "            return {\"image\": _3d_images, \"target\": target}\n",
        "\n",
        "    def load_dicom_images_3d(\n",
        "        self,\n",
        "        case_id,\n",
        "        type,\n",
        "        num_imgs=NUM_IMAGES_3D,\n",
        "        img_size=IMAGE_SIZE,\n",
        "        rotate=0,\n",
        "    ):\n",
        "        case_id = str(case_id).zfill(5)\n",
        "\n",
        "        path = f\"/content/folder/{self.folder}/{case_id}/{type}/*.dcm\"\n",
        "        files = sorted(\n",
        "            glob.glob(path),\n",
        "            key=lambda var: [\n",
        "                int(x) if x.isdigit() else x for x in re.findall(r\"[^0-9]|[0-9]+\", var)\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        middle = len(files) // 2\n",
        "        num_imgs2 = num_imgs // 2\n",
        "        p1 = max(0, middle - num_imgs2)\n",
        "        p2 = min(len(files), middle + num_imgs2)\n",
        "        image_stack = [load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]\n",
        "\n",
        "        img3d = np.stack(image_stack).T\n",
        "        if img3d.shape[-1] < num_imgs:\n",
        "            n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n",
        "            img3d = np.concatenate((img3d, n_zero), axis=-1)\n",
        "\n",
        "        if np.min(img3d) < np.max(img3d):\n",
        "            img3d = img3d - np.min(img3d)\n",
        "            img3d = img3d / np.max(img3d)\n",
        "\n",
        "        return np.expand_dims(img3d, 0)"
      ],
      "metadata": {
        "id": "rRqo_zWym04e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того, чтобы реализовать мультимодальность в виде early fusion. Вам необходимо добавить загрузку другого вида данных. Для этого можно поменять тип загружаемого файла и сконкатенировать два тензора. Таким образом, у вас получится early fusion."
      ],
      "metadata": {
        "id": "wq41NgvIORgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создаем модель"
      ],
      "metadata": {
        "id": "cbeiCPZf-t3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = mn.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(\n",
        "    device\n",
        ")\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)"
      ],
      "metadata": {
        "id": "ELm0L24r-t-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение и оценка на валидационном наборе\n",
        "\n"
      ],
      "metadata": {
        "id": "Ds3JUVSy-0WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = BrainRSNADataset(data=df_train, mri_type=type_, is_train=True)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_ds, batch_size=2, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "val_ds = BrainRSNADataset(data=df_val, mri_type=type_, is_train=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        val_ds, batch_size=2, shuffle=False, num_workers=2\n",
        "    )"
      ],
      "metadata": {
        "id": "QvXLpkaLoCeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_interval = 1\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "\n",
        "\n",
        "wandb.init(entity=\"Your entity\", project=\"Your project\")  # Define your wandb entity\n",
        "config = wandb.config\n",
        "config.learning_rate = lr\n",
        "config.batch_size = bs\n",
        "config.mode = \"3D\"\n",
        "config.backbone = \"DenseNet121\"\n",
        "config.optimizer = \"Adam\"\n",
        "config.sampler = \"True\"\n",
        "config.resample = \"True\"\n",
        "config.registration = \"True\"\n",
        "config.normalization = \"Per patient\"\n",
        "config.epochs = epochs\n",
        "config.augmentation = \"Affine\"\n",
        "wandb.run.name = f\"BraTS_3D_Densenet121_{lr}_bs{bs}_2Channel(1c_seg)_normalization_pp\"\n",
        "wandb.watch(model)\n",
        "\n",
        "for i, epoch in enumerate(tqdm(range(epochs))):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    val_epoch_loss = 0\n",
        "    step = 0\n",
        "    val_step = 0\n",
        "    for batch_data in tqdm(train_loader):\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"target\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_len = (len(train_ds) // train_loader.batch_size) + 1\n",
        "        print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "        # wandb.log({\"loss\": loss})\n",
        "    epoch_loss /= step\n",
        "    # wandb.log({\"epoch_loss\": epoch_loss})\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor([], dtype=torch.long, device=device)\n",
        "        for val_data in val_loader:\n",
        "            val_step += 1\n",
        "            val_images, val_labels = val_data[\"image\"].to(device), val_data[\n",
        "                \"targets\"\n",
        "            ].to(device)\n",
        "            val_output = model(val_images)\n",
        "            val_loss = loss_function(val_output, val_labels)\n",
        "            val_epoch_loss += val_loss.item()\n",
        "            val_epoch_len = len(val_ds) // val_loader.batch_size\n",
        "            total_val_step = (len(val_ds) // bs) + 1\n",
        "            print(f\"{val_step}/{total_val_step}, val_loss: {val_loss.item():.4f}\")\n",
        "            # wandb.log({\"val_loss\": val_loss})\n",
        "            y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
        "            y = torch.cat([y, val_labels], dim=0)\n",
        "\n",
        "        val_epoch_loss /= val_step\n",
        "        print(f\"epoch {epoch + 1} average val_loss: {val_epoch_loss:.4f}\")\n",
        "        # wandb.log({\"val_epoch_loss\": val_epoch_loss})\n",
        "\n",
        "        acc_value_val = torch.eq(y_pred.argmax(dim=1), y)\n",
        "        acc_metric_val = acc_value_val.sum().item() / len(acc_value_val)\n",
        "        if acc_metric_val > best_metric:\n",
        "            best_metric = acc_metric_val\n",
        "            best_metric_epoch = epoch + 1\n",
        "            ##torch.save(model.state_dict(), \"best_metric_model_classification3d_early_fusion.pth\") Specify Directory\n",
        "            print(\"saved new best metric model\")\n",
        "        print(\n",
        "            f\"current epoch: {epoch + 1} current accuracy: {acc_metric_val:.4f} best accuracy: { best_metric:.4f}\"\n",
        "        )\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"loss\": loss,\n",
        "                \"Accuracy_val\": acc_metric_val,\n",
        "            }\n",
        "        )\n",
        "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor([], dtype=torch.long, device=device)\n",
        "\n",
        "        step = 0\n",
        "        train_step = 0\n",
        "        for train_data in train_loader:\n",
        "            train_step += 1\n",
        "            train_images, train_labels = train_data[\"image\"].to(device), train_data[\n",
        "                \"target\"\n",
        "            ].to(device)\n",
        "            train_output = model(train_images)\n",
        "            train_epoch_len = len(train_ds) // train_loader.batch_size\n",
        "            total_train_step = (len(train_ds) // bs) + 1\n",
        "            y_pred = torch.cat([y_pred, model(train_images)], dim=0)\n",
        "            y = torch.cat([y, train_labels], dim=0)\n",
        "\n",
        "        acc_value_train = torch.eq(y_pred.argmax(dim=1), y)\n",
        "        acc_metric_train = acc_value_train.sum().item() / len(acc_value_train)\n",
        "        wandb.log({\"Accuracy_train\": acc_metric_train})\n",
        "\n",
        "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
      ],
      "metadata": {
        "id": "dQbETN2t-0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "d510212d-145d-456f-e16a-a3a68471ddd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 1/26 [00:44<18:33, 44.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/27, train_loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 1/26 [00:55<23:04, 55.39s/it]\n",
            "  0%|          | 0/15 [00:55<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c60c8e757b3e>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 1 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "9XST1wZzObcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Что может делать Gradio?"
      ],
      "metadata": {
        "id": "0Uzuzm5fPVd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Один из лучших способов поделиться своей моделью машинного обучения, API или рабочим процессом обработки данных с другими — создать интерактивное приложение, которое позволит вашим пользователям или коллегам опробовать демо-версию в своих браузерах.\n",
        "\n",
        "Gradio позволяет создавать демоверсии и делиться ими, и все это на `Python`. И обычно всего за несколько строк кода! Итак, давайте начнем."
      ],
      "metadata": {
        "id": "-VauUy0_PY44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello, world!"
      ],
      "metadata": {
        "id": "hSqfYn4LPi_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы запустить `Gradio` с помощью простого примера «Hello, World», выполните следующие три шага:"
      ],
      "metadata": {
        "id": "6uLT9sEJPn_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Установите `Gradio` с помощью `pip`:"
      ],
      "metadata": {
        "id": "xqRtn3sKPogK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YVfScWMObiZ",
        "outputId": "3d1dca2a-310f-4813-a975-e82517fc0980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-3.47.1-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.0 (from gradio)\n",
            "  Downloading gradio_client-0.6.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.13)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=9734fcc356b0f8272e7aec915608a6dc3c73ddbb5898b70ea8005a68a8b3ddf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.103.2 ffmpy-0.3.1 gradio-3.47.1 gradio-client-0.6.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 orjson-3.9.8 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uvicorn-0.23.2 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Запустите приведенный ниже код:"
      ],
      "metadata": {
        "id": "UNIgmmZ0PswO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "qRB_Rx6cPs1U",
        "outputId": "d0a84677-42e5-4a65-97cd-5dbea4752df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://416d8145765453cee9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://416d8145765453cee9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Приведенная ниже демонстрационная версия автоматически появится в выводе блока кода или можете перейти по предложенной ссылке."
      ],
      "metadata": {
        "id": "eqxhk6qsP7cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если при локальной разработке вы хотите запустить код как скрипт `Python`, вы можете использовать `Gradio CLI` для запуска приложения в режиме перезагрузки, что обеспечит плавную и быструю разработку. Узнайте больше о перезагрузке в [Руководстве по автоматической перезагрузке](https://www.gradio.app/guides/developing-faster-with-reload-mode).\n",
        "\n",
        "```\n",
        "gradio app.py\n",
        "```\n",
        "> Примечание: вы также можете использовать `python app.py`, но он не обеспечит механизм автоматической перезагрузки."
      ],
      "metadata": {
        "id": "S6h8Z3oHQDcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Класс Interface"
      ],
      "metadata": {
        "id": "HJvUIVCFQcFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы заметите, что для создания демо мы создали `gr.Interface`. Этот класс интерфейса может обернуть любую функцию `Python` пользовательским интерфейсом. В приведенном выше примере мы видели простую текстовую функцию, но эта функция может быть чем угодно: от музыкального генератора до налогового калькулятора и функции прогнозирования предварительно обученной модели машинного обучения.\n",
        "\n",
        "Базовый класс интерфейса инициализируется тремя обязательными параметрами:\n",
        "\n",
        "* **fn**: функция для обертывания пользовательского интерфейса\n",
        "* **входные данные**: какой компонент(ы) использовать для ввода (например, **\"text\"**, **\"image\"** или **\"audio\"**)\n",
        "* **выходные данные**: какой компонент(ы) использовать для вывода (например, «текст», «изображение» или «метка»)\n",
        "\n",
        "Давайте подробнее рассмотрим эти компоненты, используемые для обеспечения ввода и вывода."
      ],
      "metadata": {
        "id": "u2EeagxdQsPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Атрибуты компонентов"
      ],
      "metadata": {
        "id": "6-NDoEP7RI3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В предыдущих примерах мы видели несколько простых компонентов `Textbox`, но что, если вы хотите изменить внешний вид или поведение компонентов пользовательского интерфейса?\n",
        "\n",
        "Допустим, вы хотите настроить текстовое поле ввода — например, вы хотите, чтобы оно было больше и имело текстовый заполнитель. Если мы будем использовать реальный класс для `Textbox` вместо использования ярлыка строки, у вас будет доступ к гораздо большим возможностям настройки через атрибуты компонента."
      ],
      "metadata": {
        "id": "50KsjRjRRJjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Name Here...\"),\n",
        "    outputs=\"text\",\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "U3Zb53aRP7kz",
        "outputId": "4e2b7310-5425-41a6-d465-f3009fc5f78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://10c2e1c9a76e4d3174.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://10c2e1c9a76e4d3174.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Множественные компоненты ввода и вывода"
      ],
      "metadata": {
        "id": "oexQNlFpRgHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предположим, у вас есть более сложная функция с несколькими входами и выходами. В приведенном ниже примере мы определяем функцию, которая принимает строку, логическое значение и число и возвращает строку и число. Посмотрите, как вы передаете список входных и выходных компонентов."
      ],
      "metadata": {
        "id": "XePk_IfxRgQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name, is_morning, temperature):\n",
        "    salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
        "    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n",
        "    celsius = (temperature - 32) * 5 / 9\n",
        "    return greeting, round(celsius, 2)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n",
        "    outputs=[\"text\", \"number\"],\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "0oLQfY6PRqnm",
        "outputId": "001d3eaa-a3a5-4157-d8b8-9f01aeae5987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://cc2792d4d3ad428dd3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cc2792d4d3ad428dd3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы просто заключаете компоненты в список. Каждый компонент в списке входов соответствует одному из параметров функции по порядку. Каждый компонент в списке выходов соответствует одному из значений, возвращаемых функцией, опять же по порядку."
      ],
      "metadata": {
        "id": "Ed_-wzexR5dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Пример изображения"
      ],
      "metadata": {
        "id": "ZtAsEcd4SB65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Gradio` поддерживает множество типов компонентов, таких как изображение, датафрейм, видео или метка. Давайте попробуем функцию преобразования изображения в изображение, чтобы почувствовать это!"
      ],
      "metadata": {
        "id": "EqRSgpL2SH0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "def sepia(input_img):\n",
        "    sepia_filter = np.array([\n",
        "        [0.393, 0.769, 0.189],\n",
        "        [0.349, 0.686, 0.168],\n",
        "        [0.272, 0.534, 0.131]\n",
        "    ])\n",
        "    sepia_img = input_img.dot(sepia_filter.T)\n",
        "    sepia_img /= sepia_img.max()\n",
        "    return sepia_img\n",
        "\n",
        "demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "Gxr9EVEHSMVB",
        "outputId": "107c85d0-92b2-4d1d-ef71-6d55a604847a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3b664b801f396fe275.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3b664b801f396fe275.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При использовании компонента Image в качестве входных данных ваша функция получит массив `numpy` вида `(height, width, 3)`, где последнее измерение представляет значения RGB. Мы также вернем изображение в виде массива `numpy`.\n",
        "\n",
        "Вы также можете установить тип данных, используемый компонентом, с помощью аргумента ключевого слова `type=`. Например, если вы хотите, чтобы ваша функция использовала путь к файлу изображения вместо массива `numpy`, входной компонент изображения можно записать так:\n",
        "```\n",
        "gr.Image(type=\"filepath\", shape=...)\n",
        "```\n",
        "\n",
        "Также обратите внимание, что наш входной компонент «Изображение» имеет кнопку редактирования 🖉, которая позволяет обрезать и масштабировать изображения. Манипулирование изображениями таким образом может помочь выявить предвзятости или скрытые недостатки в модели машинного обучения!\n",
        "\n",
        "Вы можете прочитать больше о многих компонентах и о том, как их использовать, в [документации Gradio](https://www.gradio.app/docs/interface)."
      ],
      "metadata": {
        "id": "scQ_3NUGShO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Чатботы"
      ],
      "metadata": {
        "id": "cgpFcBFaTwUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio включает класс высокого уровня `gr.ChatInterface`, который похож на gr.Interface, но специально разработан для пользовательских интерфейсов чат-ботов. Класс `gr.ChatInterface` также является оберткой для функции, но эта функция должна иметь определенную сигнатуру. Функция должна принимать два аргумента: `message`, а затем `history` (аргументы могут быть названы как угодно, но должны быть в указанном порядке)\n",
        "* `message`: строка, представляющая ввод пользователя\n",
        "* `history`: список, представляющий разговоры до этого момента. Каждый внутренний список состоит из двух строк, представляющих пару: [user input, bot response].\n",
        "\n",
        "Ваша функция должна возвращать однострочный ответ, который является ответом бота на конкретное входное сообщение пользователя.\n",
        "\n",
        "Помимо этого, `gr.ChatInterface` не имеет обязательных параметров (хотя некоторые из них доступны для настройки пользовательского интерфейса).\n",
        "\n",
        "Вот игрушечный пример:"
      ],
      "metadata": {
        "id": "iuBgE5H4T_ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gradio as gr\n",
        "\n",
        "def random_response(message, history):\n",
        "    return random.choice([\"Yes\", \"No\"])\n",
        "\n",
        "demo = gr.ChatInterface(random_response)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "zp_Wej1QTa-c",
        "outputId": "dc5395dc-c06c-47ec-b52a-981244fb3aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://833627dd51b61ce5a9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://833627dd51b61ce5a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подробнее о `gr.ChatInterface` можно прочитать [здесь](https://www.gradio.app/guides/creating-a-chatbot-fast)."
      ],
      "metadata": {
        "id": "d0aT0ht2UCLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blocks: больше гибкости и контроля"
      ],
      "metadata": {
        "id": "dbGRJPuzUCOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio предлагает два подхода к созданию приложений:\n",
        "\n",
        "1. `Interface` и `ChatInterface`, которые обеспечивают высокоуровневую абстракцию для создания демонстраций, которые мы обсуждали до сих пор.\n",
        "\n",
        "2. `Blocks` — низкоуровневый API для разработки веб-приложений с более гибкими макетами и потоками данных. Блоки позволяют вам выполнять такие действия, как отображение нескольких потоков данных и демонстраций, контролировать появление компонентов на странице, обрабатывать сложные потоки данных (например, выходные данные могут служить входными данными для других функций) и обновлять свойства/видимость компонентов на основе взаимодействия с пользователем. Если эта возможность настройки вам нужна, попробуйте вместо этого `Blocks`!"
      ],
      "metadata": {
        "id": "h_IStPIvVvSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hello, Blocks!"
      ],
      "metadata": {
        "id": "yLoHeU_vWKFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте рассмотрим простой пример. Обратите внимание, насколько здесь API отличается от `Interface`."
      ],
      "metadata": {
        "id": "h7HvcmKcWQ40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    name = gr.Textbox(label=\"Name\")\n",
        "    output = gr.Textbox(label=\"Output Box\")\n",
        "    greet_btn = gr.Button(\"Greet\")\n",
        "    greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "ku3YlAN9WKK3",
        "outputId": "68966b42-cd8b-480a-85fe-d0bde4d9e6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0294319c049b2f1d6c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0294319c049b2f1d6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что следует отметить:\n",
        "\n",
        "* `Blocks` создаются с помощью предложения `with`, и любой компонент, созданный внутри этого предложения, автоматически добавляется в приложение.\n",
        "* Компоненты отображаются в приложении вертикально в том порядке, в котором они созданы.\n",
        "* Была создана кнопка, а затем к этой кнопке был добавлен event-listener `click`. API для этого должен выглядеть знакомо! Как и `Interface`, метод `click` принимает функцию `Python`, компоненты ввода и компоненты вывода."
      ],
      "metadata": {
        "id": "_40ov2CKWnDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Больше сложности"
      ],
      "metadata": {
        "id": "dUUlsTe2XMTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот приложение, которое даст вам представление о возможностях `Blocks`:"
      ],
      "metadata": {
        "id": "PcNqJ9twXSYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def flip_text(x):\n",
        "    return x[::-1]\n",
        "\n",
        "\n",
        "def flip_image(x):\n",
        "    return np.fliplr(x)\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"Flip text or image files using this demo.\")\n",
        "    with gr.Tab(\"Flip Text\"):\n",
        "        text_input = gr.Textbox()\n",
        "        text_output = gr.Textbox()\n",
        "        text_button = gr.Button(\"Flip\")\n",
        "    with gr.Tab(\"Flip Image\"):\n",
        "        with gr.Row():\n",
        "            image_input = gr.Image()\n",
        "            image_output = gr.Image()\n",
        "        image_button = gr.Button(\"Flip\")\n",
        "\n",
        "    with gr.Accordion(\"Open for More!\"):\n",
        "        gr.Markdown(\"Look at me...\")\n",
        "\n",
        "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
        "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "O7GlCHX2XMZd",
        "outputId": "344ef9c3-7af8-4528-9aab-cb0e29ab2e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://9c24a8c42feb0b9ba0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9c24a8c42feb0b9ba0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поздравляем, теперь вы знакомы с основами `Gradio`! Чтобы узнать больше о ключевых функциях `Gradio`, можете перейти к [руководству](https://www.gradio.app/guides/key-features)."
      ],
      "metadata": {
        "id": "tdT_8UZ6XdVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выводы:\n",
        "\n",
        "В этом семинаре мы познакомились с тем, где на практике можно применять мультимодальность - медицинские данные. Мультимодальность на многих задачах может добавлять хорошее качество (например, при разработке моделей для беспилотников) поэтому работать с ней один из наинужнейших навыков будущего.\n",
        "\n",
        "Во второй части мы познакомились с тем, как можно хостить ваши модели в виде простых демок через пакет gradio. Умение создать такие небольшие MVP быстро выделит вас в вашей компании :)"
      ],
      "metadata": {
        "id": "Mn18emV2PDFc"
      }
    }
  ]
}